\documentclass[]{ceurart}

\usepackage{soul}
\usepackage{caption}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{array}
\usepackage{multirow}
%\usepackage{algorithm}
%\usepackage{algpseudocode}
\usepackage[linesnumbered]{algorithm2e}
\usepackage{amsmath}

\lstset{breaklines=true}

\newcommand\ty[1]{\textsf{#1}}
\newcommand\sym[1]{\textsf{#1}}
\newcommand\var[1]{\mathit{#1}}

\newdefinition{definition}{Definition}
\newdefinition{example}[definition]{Example}

\begin{document}

\copyrightyear{2024}
\copyrightclause{Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0)}

\conference{Submitted draft} % TODO update

% TODO, at the end check the following
%   - formulas -> formulae
%   - clause -> formula
%   - monomorphization -> monomorphisation
%   - higer order -> higer-order
%   - TH1 with ugly 1
%   - correctly spelled TPTP, Leo-III, Zipperposition, E, Vampire
%   - use of formula vs clause
%   - polymorphic and monomorphic
%   - title of bits of the algorithms
%   - eg or ex -> e.g.
%   - function symbol -> symbol instance (depends on context)
%   - ... -> \dots
%   - comma splices!!!!
%   - ``'' -> `'
%   - use \Phi instead of F for formulae
%   - concrete types -> monomorphic types
%   - use in not \in when we are not precisely dealing with set membership
%   - $\Phi$ for problem and $\varphi_1$ etc.
%   - n-uples -> n-tuples
%   - ize -> ise (in cas i forgot any)
%   - remove vertical space before lists (if allowed)
%   - add full stops to end of sentences in itemize
%   - B\"ome -> B\"ohme

\title{Iterative Monomorphisation}

\author[1,2]{Tanguy Bozec}[%
email=tanguy.bozec@ens-paris-saclay.fr,
]
\author[2]{Jasmin Blanchette}[%
email=jasmin.blanchette@ifi.lmu.de,
%url=https://www.tcs.ifi.lmu.de/mitarbeiter/jasmin-blanchette_de.html,
]
\address[1]{ENS Paris-Saclay, Université Paris-Saclay, France}
\address[2]{Institute of Informatics, Ludwig-Maximilians-Universität München, Germany}


\begin{abstract}
Monomorphisation can be used to extend monomorphic provers to support polymorphic logics. We propose an iterative approach, which is necessarily incomplete but which works well in practice. It is implemented in the Zipperposition prover, where it can be used to translate away polymorphism before invoking the monomorphic prover E as a backend. Our evaluation demonstrates that this approach increases Zipperposition's success rate. Moreover, we find iterative monomorphisation to be competitive with native implementations of polymorphism.
\end{abstract}

\begin{keywords}
   Polymorphism\sep
   monomorphism\sep
   automatic theorem proving
\end{keywords}

\maketitle

% target page number: 2-2.5
\section{Introduction}

One of the main applications of automatic theorem provers is to provide automation to users of proof assistants. Many proof assistants, such as HOL4 \cite{slind-norrish-2008}, HOL Light \cite{harrison-2009}, and
Isabelle/HOL \cite{nipkow-et-al-2002}, support rank-1 polymorphism, where type quantification is allowed only at the top level of formulae. On the other hand, many automatic provers operate only on monomorphic logics. One approach to close this gap is to extend provers to natively support polymorphism, as has been done for Vampire \cite{bhayat-reger-2020}. This, however, entails a lot of work that needs to be redone for every prover.

  % apps are rank-1 polymorphic, but provers are mono
  % there's a gap
  % one solution is to extend the provers to support polymorphism;
  % this was done e.g. for Vampire \cite{xxx}
  % but this is a lot of work, and needs to be done for every prover

The alternative is to translate polymorphic problems to monomorphic problems. One approach is to encode polymorphism using dedicated function or predicate symbols \cite{mono-trans} in a monomorphic logic. Another approach is to encode polymorphism using {iterative monomorphisation}, as introduced by B\"ohme \cite[Section 2.2.1]{sb-phd}. This method relies on heuristically instantiating the formulae's type variables with concrete types.

 % * the alternative is to translate polymorphic problems to monomorphic problems
 %   * two main approaches:
 %     * complete encoding of polymorphism, using guards or tags,
 %       as in Blanchette et al.~\cite{mono-trans} and Bobot et al.~\cite{expr-poly-types}
 %     * incomplete encoding of polymorphism based on iterative monomorphisation
 %       such as introduced by Böhme \cite[Section 2.2.1]{sb-phd}, whereby type
 %       variables are heuristically instantiated by concrete types

By a type version of the compactness theorem, we have that in first order logic, given a polymorphic formula \(\varphi\), there exist an equisatisfiable finite set of monomorphic instances of \(\varphi\). However, such a set cannot be computed \cite[Theorem 1]{expr-poly-types}. As a result, any monomorphisation method based on instantiation of type variables is
bound to be incomplete.

  %* By a type version of compactness, if a set of polymorphic
  %  formulae is unsatisfiable, then there exists a finite set of monomorphic
  %  instances of the formulae that is also unsatisfiable. However, this
  %  finite set cannot be computed \cite[Section 2, Theorem 1]{expr-poly-types}; hence any approach based on
  %  instantiation of type variables is incomplete.


B\"ohme's iterative approach is implemented as part of the SMT (satisfiability modulo theories) integration \cite[Chapter 2]{sb-phd} in Isabelle/HOL. This implementation is also used by Sledgehammer \cite{judgement, hammer} to interface with superposition based automatic theorem provers. However, it is is documented only as a single subsection in his PhD thesis \cite[Section 2.2.1]{sb-phd}.

In this paper, we present an algorithm based on our understanding of his description and implementation (Section~\ref{sec:high-level-algorithm}). We also provide a more detailed description to help future implementers. In addition, this description shows some of the ways in which an implementation can be made more efficient and avoid explosions (Section~\ref{sec:low-level-algorithm}).

% * Böhme's iterative approach is implemented as part of the SMT integration \cite[Chapter 2]{sb-phd}
%     in Isabelle/HOL. This implementation is also used by
%    Sledgehammer \cite{judgement, hammer} to interface automatic theorem provers based on
%    superposition.
%
%  * The main drawback of Böhme's approach is that it is only briefly documented,
%    as a single paragraph in his PhD thesis \cite[Section 2.2.1]{sb-phd}
%  * In this paper, we present an algorithm inspired by his description and
%    implementation (Section~\ref{sec:high-level-algorithm}).
%    * we aimed at mathematical precision, so that people who want to reimplement
%      the algorithm can follow this blueprint
%    * we also show how to bound the algorithm to avoid explosions in practice
%      (Section~\ref{sec:low-level-algorithm}).
%      ``the explosive nature of the procedures requires various bounds which
%      limit the number of type arguments and clauses that are generated.''


The algorithm works as follows. We assume problems to be sets of formulae. All symbols of the problem are collected, and the polymorphic instances of symbols are matched against the monomorphic ones. This yields new instances of symbols, both polymorphic and monomorphic. The process is then iterated
a number of times, making use of the newly generated instances.

%These iterations are required to account for $n$-ary type constructors.
Consider the unary type constructor \ty{list}. If a formula contains $\ty{list}(\alpha)$, where $\alpha$ is a type variable, it may be possible to generate the types $\ty{list}(\ty{int})$, $\ty{list}(\ty{list}(\ty{int}))$, etc. However, because new types emerge through matching, $\ty{list}(\ty{list}(\ty{int}))$ can be obtained only once the $\ty{list}(\ty{int})$ instance has already been generated. This example also makes it clear that the set of instances that can be generated is infinite.

To keep the number of generated formulae finite, we limit the number of iterations. After the iterations are completed, the new monomorphic symbol instances are used to instantiate the polymorphic symbols in the problem's formulae, generating new monomorphic formulae. Finally, because monomorphic provers generally do not support $n$-ary type constructors, types must be `mangled'; for example, the compound type $\ty{list}(\ty{int})$ could be mangled to the constant $\ty{list\_int}$.

We implemented iterative monormorphisation in Zipperpostion \cite{zipp}, a higher order prover written in OCaml. Although Zipperposition is polymorphic, it uses the monomorphic prover E \cite{e} as a backend. This means that E can now be used with polymorphic problems. Moreover, our implementation of the algorithm in Zipperposition can be used as a preprocessor to interface with other stand-alone provers.

Our empirical evaluation on TPTP \cite{tptp} problems attempts to answer three questions (Section~\ref{sec:evaluation}):
%
\begin{enumerate}
\item Is the new Zipperposition with the E backend more successful on polymorphic problems than Zipperposition without backend?

\item How competitive are monomorphic provers on monomorphised polymorphic problems?

\item Is iterative monomorphisation more effective than the native polymorphism implemented in polymorphic provers?
\end{enumerate}

Our findings are as follows:
%
\begin{enumerate}
\item Zipperposition benefits substantially from the E backend.

\item E with monomorphisation comes close second to the polymorphic prover Vampire.

\item For Leo-III \cite{leo-iii} and Vampire \cite{vamp}, we find that monomorphisation is indeed more effective than native polymorphism.
\end{enumerate}

% target page number: 0.5-1.5
\section{Preliminaries}
\label{sec:preliminaries}

This algorithm works independently of the structure of the problem's formulae. It relies exclusively on the formulae's monomorphic and polymorphic symbol instances. Type variables are assumed to be implicitly universally quantified at a formula's top level. The precise form of formulae is left unspecified.
%
Due to this generality, iterative monomorphisation can be used with any reasonable rank-1 polymorphic logic. In particular, it can operate in the polymorphic first- and higher-order logics embodied by TPTP's TF1 and TH1 syntaxes \cite{blanchette-paskevich-2013,th1}, implemented by several automatic provers.

%The following terms are used in the algorithm descriptions.

%{\obeylines
%
%\noindent\kern\parindent
%  * algorithm works a priori with any rank-1 polymorphic logic
%
%  * in particular, it works with polymorphic first- and higher-order logic as 
%    embodied by TPTP's TF1 and TH1 syntaxes \cite{blanchette-paskevich-2013,th1}
%
%  * the algorithm does not need to inspect the precise structure of formulae
%  * instead, it works on monomorphic or polymorphic instances of symbols that 
%    occur in the formulae
%  * type quantification is implicit, meaning that all type variables occurring
%    in a formula are assumed to be universally quantified at the formula's top
%    level
%}

% TH1 ugly for height of 1 reasons, can be fixed with case feature

Our abstract framework relies on the following basic definitions.

\begin{definition}%[Type]
A (\emph{polymorphic}) \emph{type} \(\tau\) can be a type variable (e.g.\ \(\alpha\)) or
the application of an \(n\)-ary type constructor to \(n\) types (e.g.\ \(\ty{list}(\alpha)\), \(\ty{map}(\ty{int},\ty{string})\)).
If $n = 0$, we omit the parentheses (e.g.\ \(\ty{int}\)).
\end{definition}

\begin{definition}%[Monomorphism and polymorphism]
A type is \emph{monomorphic} if it contains no type variables.
\end{definition}

\begin{definition}%[Function symbol]
A (function or predicate) {symbol} \(f\) has a \emph{type arity} that defines the number of type arguments it takes. A \emph{symbol instance} is a symbol applied to type arguments listed between angle brackets: \(f\langle \tau_1, \dots, \tau_n\rangle\), where each $\tau_i$ is a type. If $n = 0$, we omit the brackets (e.g.\ $f$).
\end{definition}

\begin{definition}
A (\emph{type}) \emph{substitution} is a partial function mapping a finite number of type variables to corresponding types. Substitutions are written as
$\sigma = \{\alpha_1\mapsto\tau_1, \dots, \alpha_n\mapsto\tau_n\}$. They are assumed to be lifted to formulae; thus, $\sigma(\varphi)$ yields the variant of $\varphi$ in which each $\alpha_i$ is replaced by $\tau_i$.
Given two substitutions \(\tau, \upsilon\), the successive application of \(\tau\) and \(\upsilon\) is denoted by \(\upsilon \circ \tau\).
\end{definition}

\begin{definition}
Two substitutions \(\{\alpha_1 \mapsto \tau_1, \dots, \alpha_m\mapsto\tau_m\}\) and \(\{\beta_1 \mapsto \upsilon_1, \dots, \beta_n\mapsto\upsilon_n\}\) are \emph{compatible} if \(\alpha_i = \beta_j\) implies \(\tau_i = \upsilon_j\) for all \(i, j\).
\end{definition}


\begin{definition}
Given two types \(\tau, \upsilon\), \emph{matching} \(\upsilon\) against \(\tau\) will either fail or yield a substitution \(\sigma\) such that \(\sigma(\upsilon) = \tau\).
\end{definition}

In the following sections, we will always match a polymorphic type $\upsilon$ against a monomorphic type $\tau$.

% target page number: 2-4
\section{High level algorithm}
\label{sec:high-level-algorithm}

%We start with a high level description of the iterative monomorphisation algorithm.
The iterative monomorphisation algorithm takes a polymorphic problem as input and returns a monomorphic problem. It operates by applying an arbitrary number of iterations. Each iteration takes a polymorphic problem as argument and returns a problem with additional partially instantiated formulae. A single iteration consists of a collection phase
% (Section~\ref{ssec:collection})
and an instantiation phase.
% (Section~\ref{ssec:instantiation})
Once the iterations are completed, a final step filters out all non-monomorphic formulae returned by the last iteration.

%\subsection{Collection}
%\label{ssec:collection}

The initial phase of each iteration consists of computing two maps, \(M\) and \(P\), from the input problem~$\Phi$.
%
\begin{enumerate}
\item[\labelitemi] Given a symbol \(f\) occurring in \(\Phi\), the set \(M(f)\) consists of all tuples of monomorphic type arguments to which \(f\) is applied in \(\Phi\). For example, if \(\sym{foldl}\langle \ty{nat}, \ty{int}\rangle\) occurs in \(\Phi\), then \((\ty{nat}, \ty{int}) \in M(\sym{foldl}) \).

\item[\labelitemi] Given a formula \(\varphi \in \Phi\) and a symbol \(f\) occurring in \(\varphi\), the set \(P(\varphi, f)\) consists of all tuples of type arguments to which \(f\) is applied in \(\varphi\) and which contains a type variable. For example, if \(\sym{foldl}\langle \ty{nat}, \ty{list}(\alpha)\rangle\) occurs in \(\varphi\), then \((\ty{nat}, \ty{list}(\alpha)) \in P(\varphi, \sym{foldl}) \).
\end{enumerate}

It is important to parametrise \(P\) with \(\varphi\) because type variables are quantified at the formula level. The formula indicates the scope of type variables. This is not necessary for \(M\) since all the types it contains are monomorphic.

%\subsection{Instantiation}
%\label{ssec:instantiation}

Once the maps \(M\) and \(P\) are initialised, each iteration performs the following steps to create new instances of formulae:

\begin{enumerate}

   \item Create an empty set of formulae \(\Phi'\).

   \item For each formula \(\varphi \in \Phi\) and for each symbol \(f\) occuring in \(\varphi\):
   \begin{enumerate}
    \item[2.1.] For each tuple \((\tau_1, \dots, \tau_n) \in  M(f)\)        and each tuple \((\upsilon_1, \dots, \upsilon_n) \in P(\varphi, f)\),
     for each \(i\), match \(\upsilon_i\) against \(\tau_i\), yielding the substitution \(\sigma_i\) in case of success.

    \item[2.2.] If all \(n\) matchings are successful and the substitutions \(\sigma_i\) are pairwise compatible,
add the formula \((\sigma_1 \circ \dots \circ \sigma_n)(\varphi)\) to \(\Phi'\).
   \end{enumerate}

   \item Return \(\Phi \cup \Phi'\).

\end{enumerate}

The algorithm is clearly sound because the newly generated formulae are instances of the initial problem's formulae, where type variables have been instantiated with monomorphic types. It is, however, not guaranteed to be complete.

%\subsection{Example}

\begin{example}Consider the following problem:
\begin{enumerate}
   \item \(\sym{p}\langle \ty{int}\rangle(0)\)
   \item \(\forall a: \alpha{,}\; \mathit{as}:\ty{list}(\alpha){,}\; \sym{p}\langle\alpha\rangle(a) \longrightarrow \sym{p}\langle \ty{list}(\alpha)\rangle(\mathit{as})\)
\end{enumerate}
%
The first iteration matches \(\alpha\) against \(\ty{int}\) for $\sym{p}$, generating the formula
%
\begin{enumerate}
   \item[3.] \(\forall a: \ty{int}{,}\; \mathit{as}:\ty{list}(\ty{int}){,}\; \sym{p}\langle\ty{int}\rangle(a) \longrightarrow \sym{p}\langle \ty{list}(\ty{int})\rangle(\mathit{as})\)
\end{enumerate}
%
The second iteration matches \(\alpha\) against \(\ty{list}(\ty{int})\), leading to the formula
%
\begin{enumerate}
   \item[4.] \(\forall a: \ty{list}(\ty{int}){,}\; \mathit{as}:\ty{list}(\ty{list}(\ty{int})){,}\; \sym{p}\langle\ty{list}(\ty{int})\rangle(a) \longrightarrow \sym{p}\langle \ty{list}(\ty{list}(\ty{int}))\rangle(\mathit{as})\)
\end{enumerate}
%
Similarly the third iteration adds
%
\begin{enumerate}
   \item[5.] \(\forall a: \ty{list}(\ty{list}(\ty{int})){,}\; \mathit{as}:\ty{list}(\ty{list}(\ty{list}(\ty{int}))){,}\; \sym{p}\langle\ty{list}(\ty{list}(\ty{int}))\rangle(a) \longrightarrow \sym{p}\langle \ty{list}(\ty{list}(\ty{list}(\ty{int})))\rangle(\mathit{as})\)
\end{enumerate}

%
%
%On the first iteration, two matchings are attempted, that of \(\alpha\) against \(\ty{int}\) which yields \(\sigma = \{\alpha \mapsto \ty{int}\}\) and that of \(\ty{list}(\alpha)\) against \(\ty{int}\) which fails. Then, \(\sigma\) is applied to [1], resulting in a new formula: \(\forall a: \ty{int}, \mathit{as}:\ty{list}(\ty{int}), \sym{p}\langle\ty{int}\rangle(a) \longrightarrow \sym{p}\langle \ty{list}(\ty{int})\rangle(\mathit{as})\)
%
%At the beginning of the second iteration, the clause generated in the previous iteration is used to update \(M(f)\), adding \((\ty{list}(\ty{int}))\). \(P\) has no new elements. Similarly to the first iteration we attempt the following matchings:
%\begin{itemize}
%   \item \(\alpha\) against \(\ty{int}\) which succeeds resulting in \(\sigma\)
%   \item \(\ty{list}(\alpha)\) against \(\ty{int}\) which fails
%   \item \(\alpha\) against \(\ty{list}(\ty{int})\) resulting in \(\sigma'=\{\alpha\mapsto\ty{list}(\ty{int})\}\)
%   \item \(\ty{list}(\alpha)\) against \(\ty{list}(\ty{int})\) resulting in \(\sigma\)
%\end{itemize}
%
%From these substitutions, a single new formula can be generated from the application of \(\sigma'\) to [1], giving us: \(\sigma'(\text{[1]}) = \forall a: \ty{list}(\ty{int}), \mathit{as}:\ty{list}(\ty{list}(\ty{int})), \sym{p}\langle\ty{list}(\ty{int})\rangle(a) \longrightarrow \sym{p}\langle \ty{list}(\ty{list}(\ty{int}))\rangle(\mathit{as})\)
%
%This new formula will be used to add \((\ty{list}(\ty{list}(\ty{int})))\) to \(M(f)\) at the beginning of the third iteration. Similarly to the previous iteration, a single new formula will be generated. It is obtained by applying the new substitution \(\sigma'' = \{\alpha\mapsto\ty{list}(\ty{list}(\ty{int}))\}\) to [1].
\end{example}

This example illustrates how an infinite number of new formulae can be generated from a simple initial problem. Although the example may seem artificial, similar problems frequently arise in practice. For example, the \sym{concat} function of Isabelle/HOL \cite{xxx}, which is characterised in the base case by the equation \(\sym{concat}\langle\alpha\rangle\; \sym{Nil}\langle\ty{list}(\alpha)\rangle = \sym{Nil}\langle\alpha\rangle\), exhibits the same behavior. Any reasonable implementation requires bounds limiting the number of new type arguments, substitutions and formulae to be useful.

% target page number: 2-4
\section{Low level algorithm}
\label{sec:low-level-algorithm}

This section presents a detailed description of a practical adaptation of the high-level algorithm, it closely follows the implementation in Zipperposition. There are several significant departures from the high-level algorithm:
\begin{itemize}
   \item substitutions are applied to the type arguments, instead of the formulae. This dispenses from having to extract type arguments from the formulae each iteration
   \item new formulae are generated only once all substitutions have been computed in a separate step
   \item bounds are used at several stages of the algorithm as a means to curb the highly explosive enumerations
   \item type arguments are separated into old and new sets, avoiding the computation of the same matchings each iteration
   \item once the new formulae are generated, their monomorphic types are mangled before the end result is output
\end{itemize}

The maps \(P\) and \(M\) are assumed to have been initialised from the initial problem. This step depends on the precise form of the input formulae which has been left undefined.

\begin{algorithm}[H]
   \LinesNumbered
   \SetKw{And}{and}\SetKw{True}{true}\SetKw{False}{false}
   \SetKwData{Continue}{continue}\SetKwData{SubstRes}{substitution\_result}
   \SetKwFunction{Success}{Success}\SetKwFunction{Compatible}{compatible}
   \SetKwFunction{Match}{match}
   \SetKwInOut{Input}{input}\SetKwInOut{Output}{output}

   \Input{\(\varphi\), \(P\), \(M\)}
   \Output{\(S\)}
   \BlankLine

   \(S\leftarrow \emptyset\)\;
   \ForEach{\(f\mapsto (\upsilon_1, \dots,\upsilon_n)\in P(\varphi)\)}{
      \ForEach{\( (\tau_1, \dots,\tau_n) \in M(f) \)}{
         \(\sigma\leftarrow \{\}\)\;
         \Continue \(\leftarrow\) \True\;
         \(i \leftarrow 0\)\;
         \While{\(i \leq n\) \And \Continue}{
            \SubstRes \(\leftarrow\) \Match{\(\upsilon_i\), \(\tau_i\)}\;
            \lIf{\SubstRes = \Success{\(\sigma_i\)} \And \Compatible{\(\sigma\), \(\sigma_i\)}}{
               \(\sigma\leftarrow\sigma_i\circ\sigma\) 
            }
            \lElse{\Continue\(\leftarrow\)\False}
            \(i\leftarrow i+1\)\;
         }
         \lIf{\Continue}{\(S\leftarrow S\cup \{\sigma\}\)}
      }

   }
   \Return \(S\)\;

\caption{Substitution Generation}
\end{algorithm}

%\begin{algorithm}[]
%\begin{algorithmic}[1]
%\Function{Substitution Generation}{Input: \(\varphi\), M, P}
%   \State new\_substitutions = \(\emptyset\)
%   \ForAll {\(f\mapsto (\upsilon_1, \dots,\upsilon_n)\in P(\varphi)\)}
%      \ForAll {\( (\tau_1, \dots,\tau_n) \in M(f) \)}
%         \State \(\sigma\) = \{\}
%         \State continue = true
%         \While {\(i\leq n\) and continue}
%            \State subtitution\_result = match(\(\upsilon_i\), \(\tau_i\))
%            \If {substitution\_result = Success(\(\sigma_i\)) and compatible(\(\sigma_i\), \(\sigma\))}
%               \State \(\sigma = \sigma \circ \sigma_i\)
%            \Else
%               \State continue = false
%            \EndIf
%
%         \EndWhile
%
%      \EndFor
%   \EndFor
%
%   \State \Return (I, U, S)
%
%\EndFunction
%\end{algorithmic}
%\end{algorithm}
%In the Zipperposition implemntation this iteration procedure is limited by several bounds. The number of new type arguments that can be generated for each symbol is limited by user-defined bounds. These bounds can be defined proportionally. Additionally, the number of new type arguments can be limited at the clause level. In both cases, the bounds for the new ground type arguments and non-ground type arguments can be set independently.
%
%A consequences of these bounds is that in order to avoid unecessary computations, the program will iteratively apply substitutions until either the ground or non-ground bound is reached. Then substitutions can be discarded depending on whether they instantiate a given set of type arguments or not.
%
%Additionally, there are bounds for the number of substitutions that are generated.
%
%\begin{algorithm}[tbh]
%\begin{algorithmic}[1]
%\Function{Instantiate clause}{Clause: C, Substitutions S}
%
%   \If{\(C\) is ground}
%
%      \State \Return \(\{C\}\)
%
%   \Else
%
%      \State let \(v\) be a type variable in \(C\)
%      \State let new\_clauses be an empty set of clauses
%
%      \ForAll{subst \(\in S(C)\)}
%
%         \If{subst instantiates \(v\)}
%
%            \State \(C' = apply(subst, C)\)
%
%            \State let v\_clauses = Instantiate clause(C', S)
%
%         \EndIf
%
%         \State \( new\_clauses = new\_clauses \cup v\_clauses\)
%
%      \EndFor
%
%      \State \Return new\_clauses
%   \EndIf
%
%\EndFunction
%\end{algorithmic}
%\end{algorithm}
%This function differs from the implementation in a significant way, indeed, through experimentation it was found that the generated clauses were far more likely to be solved by E if the substitutions that are combined to instantiate the clause were derived from the same iteration. In practice, we therefore store the iteration at which each substition is generated and at the instantiation phase only apply substitutions from the same iteration.
%
%
%What the previous function does is essentially going through the tree of possible substitutions that instantiate the clause and collecting all possible instantiations. This is extremely explosive, and a hard cap on the number of clauses we want to generate is almost indispensable for any implementation.
%
%\begin{algorithm}[tbh]
%\begin{algorithmic}[1]
%\Function{Generate clauses}{Problem: P, Substitutions: S}
%
%\ForAll {clause \(C \in P\)}
%   \State let new\_clauses = Instantiate clause(C, S)
%   \State \(P = P \cup new\_clauses\)
%\EndFor
%
%\State \(P' = \emptyset \)
%
%\ForAll {clause \(C \in P\)}
%   \If{\(C\) is ground}
%      \State \(P' = P' \cup \{mangle(C)\}\)
%   \EndIf
%\EndFor
%
%\State \Return \(P'\)
%
%\EndFunction
%\end{algorithmic}
%\end{algorithm}
%
%Implementation in Zipperposition.

% target page number: 2-4
\section{Evaluation}
\label{sec:evaluation}

% Provide complete list of problems explain which ones were excluded and why
% We ignore the following bounds: ty var limit per clause, subst per ty var, limiting the new type args per clause

\subsection{Parameter optimisation}

Considering the total number of parameters, it would be impractical to exhaustively test all option combinations. Consequently, options have been grouped together into sets of 2 or 3 options that will be more thouroughly tested. Two options will be part of the same group if it is deemed likely that a change in one of the options will significantly impact how much a change in the second option will affect the overall results.

The first two groups of options are natural as they affect how many new monomorphic and polymorphic type arguments are generated respectively for each symbol at each iteration. These options are closely linked and must be set together if they are to be relevant.

During an iteration, for each symbol (for each clause in the polymorphic case), the number of newly generated type arguments is determined by this formula:
\( \min(\text{cap}, \max(\text{floor}, \text{multiplier} \times \text{number of type arguments})) \)

The multiplier allows for a controlled increase of the number of type arguments, relative to the size of the problem whilst the floor prevents some symbols seeing no new type arguments if the multiplier is too low.
The cap acts as a final limit in the case the explosion is too fast despite the multiplier bound.

\begin{table}[th]
\caption{Evaluation of bounds for monomorphic type argument generation}
\centering\begin{tabular}{@{}l*{9}{>{\centering\arraybackslash}p{1.1em}}@{}}
   \toprule
   & &&& \multicolumn{3}{c}{floor} \\
   & \multicolumn{3}{c}{1} & \multicolumn{3}{c}{3} & \multicolumn{3}{c}{9}\\
   \cmidrule(l){2-10}
   & &&& \multicolumn{3}{c}{multiplier} \\
   \multirow{1}{2em}{cap} & 0 & 1 & 2 & 0 & 1 & 2 & 0 & 1 & 2\\
    \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(l){8-10}
    25   & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    50   & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    100  & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    \(\infty\) & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    \bottomrule
\end{tabular}
\end{table}

\bigskip
\bigskip

If the floor and multiplier are set to 0 and 0, no new polymorphic type arguments will be generated, as a result the cap will be irrelevant.

\begin{table}[th]
\caption{Evaluation of bounds for polymorphic type argument generation}
\centering\begin{tabular}{@{}l*{9}{>{\centering\arraybackslash}p{1.1em}}@{}}
   \toprule
   & &&& \multicolumn{3}{c}{floor} \\
   & \multicolumn{3}{c}{0} & \multicolumn{3}{c}{3} & \multicolumn{3}{c}{6}\\
   \cmidrule(l){2-10}
   & &&& \multicolumn{3}{c}{multiplier} \\
    \multirow{1}{2em}{cap} & 0 & 0.5 & 1 & 0 & 0.5 & 1 & 0 & 0.5 & 1 \\
    \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(l){8-10}
    10 &   & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    20 &   & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    40 &   & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    \(\infty\) & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    \bottomrule
\end{tabular}
\end{table}

\begin{table}[th]
\caption{Evaluation of bounds for substitution generation}
\centering\begin{tabular}{@{}l*{4}{>{\centering\arraybackslash}p{1em}}@{}}
   \toprule
   & \multicolumn{4}{c}{mono subst}\\
   \multirow{1}{4em}{subst cap} & 2 & 5 & 7 & 10\\
   \midrule
   50   & 0 & 0 & 0 & 0\\
   100  & 0 & 0 & 0 & 0\\
   500  & 0 & 0 & 0 & 0\\
   \(\infty\) & 0 & 0 & 0 & 0\\
   
   \bottomrule
\end{tabular}
\end{table}

This table groups two bounds that directly affect the number of clauses generated, a first one that allows at most a number of new clauses equal to `multiplier' times the original number of clauses. And the cap that gives an absolute bound over the total number of clauses generated.

The choice to also include e-timeout in this table is due to the fact that E being successful in solving a monomorphised problem in a certain time limit has been observed to be highly dependent on the number of clauses it is given.

\begin{table}[th]
\caption{Evaluation of bounds directly related to the size of the output problem}
\centering\begin{tabular}{@{}l*{9}{>{\centering\arraybackslash}p{1.1em}}@{}}
   \toprule
   & &&& \multicolumn{3}{c}{cap} \\
   & \multicolumn{3}{c}{500} & \multicolumn{3}{c}{2000} & \multicolumn{3}{c}{\(\infty\)}\\
   \cmidrule(l){2-10}
   & &&& \multicolumn{3}{c}{multiplier} \\
    \multirow{1}{5.4em}{E timeout (s)} & 1 & 2 & 3 & 1 & 2 & 3 & 1 & 2 & 3 \\
    \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(l){8-10}
    2   & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    5   & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    10  & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    \(\infty\)& 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    \bottomrule
\end{tabular}
\end{table}
The next table gathers the bounds linked to substitutions generation, the first one limits the number of monomorphising substitutions that are generated per clause during the final phase. The second one gives an absolute cap on the overall number of substitutions generated at each clause during the type argument generation.


This last table attempts to determine which are the best bounds related to the depth at which the monomorphisation problem is run as well as how long zipperposition is allowed to run before monomorphisation (and the subsequent call of E) is ran.

\begin{table}[th]
% not super happy with caption
\caption{Evaluation of parameters indirectly related to the size of the output problem}
\centering\begin{tabular}{@{}l*{4}{>{\centering\arraybackslash}p{1em}}@{}}
   \toprule
   & \multicolumn{4}{c}{loop nb} \\
   \multirow{1}{4.5em}{E call step} & 2 & 3 & 4 & 5\\
   \midrule
   0 & 0 & 0 & 0 & 0\\
   15 & 0 & 0 & 0 & 0\\
   45 & 0 & 0 & 0 & 0\\
   90 & 0 & 0 & 0 & 0\\
   
   \bottomrule
\end{tabular}
\end{table}



\subsection{Monomorphisation as preprocessor}

\begin{table}[ht]
\caption{Evaluation of native polymorphism vs.\ monomorphisation}
\centering\begin{tabular}{@{}lccc@{}}
   \toprule
   & Native & Mono & Union \\
   \midrule
   E  &   & 0 & 0 \\
   Leo-III & 0 & 0 & 0 \\
   Satallax &  & 0 & 0 \\
   Vampire & 0 & 0 & 0 \\
   Zipperposition & 0 & 0 & 0 \\[1.5\jot]
   Total & 0 & 0 & 0 \\
   \bottomrule
\end{tabular}
\end{table}

\subsection{E as Zipperposition backend}

\begin{table}[ht]
\caption{Evaluation of Zipperposition without E vs. with E}
\centering\begin{tabular}{@{}lccc@{}}
   \toprule
   & without E & with E & Union \\
   \midrule
   Zipperposition & 0 & 0 & 0 \\
   \bottomrule
\end{tabular}
\end{table}

\break

% target page number: 0.5-1
\section{Related work}
\label{sec:related-work}

  * explicitly quantified formulas a la TPTP, existentials, how to eliminate

  * Böhme
  * similar algo implemented in Why3?

% target page number: 0.5-1
\section{Conclusion}
\label{sec:conclusion}

% summary 

% Future avenues

\section*{Acknowledgements}

* Sascha Böhme

\bibliography{citations}

\end{document}
