\documentclass[]{ceurart}

%\sloppy

\usepackage{soul}
\usepackage{caption}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{array}
\usepackage{multirow}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsmath}

\lstset{breaklines=true}

\newcommand\ty[1]{\textsf{#1}}
\newcommand\sym[1]{\textsf{#1}}
\newcommand\var[1]{\mathit{#1}}

\begin{document}

\copyrightyear{2024}
\copyrightclause{Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0)}

\conference{Submitted draft} % TODO update

\title{Iterative Monomorphisation}

\author[1,2]{Tanguy Bozec}[%
email=tanguy.bozec@ens-paris-saclay.fr,
]
\author[2]{Jasmin Blanchette}[%
email=jasmin.blanchette@ifi.lmu.de,
%url=https://www.tcs.ifi.lmu.de/mitarbeiter/jasmin-blanchette_de.html,
]
\address[1]{ENS Paris-Saclay, Université Paris-Saclay, France}
\address[2]{Institute of Informatics, Ludwig-Maximilians-Universität München, Germany}


\begin{abstract}
Monomorphisation makes it possible to extend monormophic provers to support polymorphic logics. We propose an iterative approach, which is necessarily incomplete but which works well in practice. It is implemented in the Zipperposition prover, where it can be used to translate away polymorphism before invoking a monomorphic prover. Our empirical evaluation reveals that iterative monorphisation is a viable alternative to native polymorphism.
\end{abstract}

\begin{keywords}
   Polymorphism\sep
   monomorphism\sep
   automatic theorem proving
\end{keywords}

\maketitle

% target page number: 2-2.5
\section{Introduction}

{
\obeylines

  * apps are rank-1 polymorphic, but provers are mono
  * there's a gap
  * one solution is to extend the provers to support polymorphism;
    this was done e.g. for Vampire \cite{xxx}
  * but this is a lot of work, and needs to be done for every prover

\bigskip

  * the alternative is to translate polymorphic problems to monomorphic problems
    * two main approaches:
      * complete encoding of polymorphism, using guards or tags,
        as in Blanchette et al.~\cite{xxx} and Bobot et al.~\cite{xxx}
      * incomplete encoding of polymorphism based on iterative monomorphisation
        such as introduced by Böhme \cite[Section 2.2.1]{sb-phd}, whereby type
        variables are heuristically instantiated by concrete types

\bigskip

  * By a type version of Herbrand's theorem, if a set of polymorphic
    formulas is unsatisfiable, then there exists a finite set of monomorphic
    instances of the formulas that is also unsatisfiable. However, this
    finite set cannot be computed \cite{xxx}; hence any approach based on
    instantiation of type variables is incomplete.

\bigskip

  * Böhme's iterative approach is implemented as part of the SMT integration
    \cite{xxx} in Isabelle/HOL. This implementation is also used by
    Sledgehammer \cite{xxx} to interface automatic theorem provers based on
    superposition.

\bigskip

  * The main drawback of Böhme's approach is that it is only briefly documented,
    as a single paragraph in his PhD thesis \cite[Section 2.2.1]{sb-phd}
  * In this paper, we present an algorithm inspired by his description and
    implementation (Section~\ref{sec:high-level-algorithm}).
    * we aimed at mathematical precision, so that people who want to reimplement
      the algorithm can follow this blueprint
    * we also show how to bound the algorithm to avoid explosions in practice
      (Section~\ref{sec:low-level-algorithm}).
      ``the explosive nature of the procedures requires various bounds which
      limit the number of type arguments and clauses that are generated.''

}

The algorithm works as follows. We assume problems to be sets of formulas. All type arguments of the problem are collected, and the polymorphic type arguments are matched against the monomorphic ones. This yields new type arguments, both polymorphic and monomorphic. The process is then repeated using the newly derived type arguments.

These iterations are required to account for $n$-ary type constructors. Consider the unary type constructor \ty{list}. If a formula contains $\ty{list}(\alpha)$, where $\alpha$ is a type variable, it may be possible to generate the types $\ty{list}(\ty{int})$, $\ty{list}(\ty{list}(\ty{int}))$, etc. However, because new types emerge through matching, $\ty{list}(\ty{list}(\ty{int}))$ can be obtained only once $\ty{list}(\ty{int})$ has already been added to the set of monomorphic types. This example also makes it clear that the set of types that can be generated is infinite.

To keep the number of generated formulas finite, we limit the number of iterations. After the iterations are completed, the new type arguments are used to instantiate the type variables in the problem's formulas, generating new monomorphic clauses. Finally, because monomorphic provers generally do not support $n$-ary type constructors, types must be mangled. For example, the compound type $\ty{list}(\ty{int})$ could be mangled to $\ty{list\_int}$.

We implemented this algorithm in Zipperpostion, a higher order automatic theorem prover written in OCaml. Although Zipperposition is polymorphic, it uses the monomorphic prover E as a backend. Our algorithm makes it possible, for the first time, to use E with polymorphic problems. Moreover, our implementation of the algorithm in Zipperposition can be used as a preprocessor to interface with other stand-alone provers.

Our empirical evaluation on TPTP problems \cite{xxx} attempts to answer three questions (Section~\ref{sec:evaluation}):
%
\begin{enumerate}
\item Is the new Zipperposition with the E backend more successful on polymorphic problems than Zipperposition without backend?

\item How competitive are monomorphic provers on monomorphised polymorphic problems?

\item Is iterative monomophisation more effective than the native polymorphism implemented in polymorphic provers?
\end{enumerate}

We find that:
%
\begin{enumerate}
\item Zipperposition benefits substantially from the E backend.

\item E with monomorphisation comes close second to the polymorphic prover Vampire.

\item For Leo-III and Vampire, we find that monomorphisation is indeed more effective than native polymorphism.
\end{enumerate}

% target page number: 0.5-1.5
\section{Background}
\label{sec:background}

% mention alternatives, (guards, tags) and current implementation in Isabelle

Language: TF1  %TH1?

We want our starting logic to be:
\begin{itemize}
    \item higher order
    \item polymorphic: we want atomic types, type constructors and universally quantified type variables
\end{itemize}
our target logic to be the same thing except we foregoe type constructors and type variables 
we want our target logic to be a subset of the starting logic


Definitions :
\begin{itemize}
   \item type arguments % part of the logic
   \item monomorphic and polymorphic types
   \item matching % being overly precise probably uncecessary
   \item term, literal, clause  % also part logic
\end{itemize}

% target page number: 2-4
% needs to be re-written
\section{High level algorithm}
\label{sec:high-level-algorithm}

We consider a problem \(P\) to be a set of clauses \(C_1, \dots, C_n\).

Algorithm :
\begin{itemize}
    \item Let \(F\) be the set of all function symbols in \(P\).
    \item Given a function symbol \(f \in F\), we define \(I(f)\), the set of all ground type arguments that are passed to \(f\) is instantiated in \(P\).

    For instance if \(f^{a \rightarrow b}\) occurs in \(P\) (with \(a\) and \(b\) ground types) then, \(a \rightarrow b \in I(f) \).
 \item Given a clause \(C \in P\) and a function symbol \(f \in C\), we define \(U(C, f)\), the set of uninstantiated type arguments passed to \(f\) in \(C\). 

    It is important to paramatrise \(U\) with the clause of \(f\) because type variables are quantified at the clause level. This is not necessary for \(I\) because its type arguments do not contain type variables. [seems a sufficient explanation, if not clear expand further]

\end{itemize}

A single iteration of the algorithm will do the following:
\begin{itemize}
   \item For each clause \(C_i \in P\) and each function symbol \(f \in C_i\), we consider the uninstantiated type arguments \(U(C_i, f)\) these type type arguments are matched against all ground type arguments of \(I(f)\)

      [We have an issue: we don't define matching, don't think we need to go into a lot of detail]

      Each such matching may return a substitution which instantiates one or more type variables of \(C_i\) (that would also be in the type arguments of \(f\))

   \item Once all such substiutions have been computed. We apply each substitutions to the clause they were generated from. These newly created clauses are added to the problem and the newly generated type arguments (both ground and uninstantiated) will be added to \(U\) and \(I\) to be used for the next iteration.
\end{itemize}

After a set number of iterations, we take all clauses with no type variables and mangle their types [see relevant paper]. The resulting set of clauses forms a problem \(P'\) with the following properties:
\begin{itemize}
   \item \(P'\) is monomorphic [rewrite that as saying \(P'\) is in the monomorphic fragment of the original logic]
   \item if a contradiction can be derived from \(P'\), then a contradiction can be derived from \(P\) [soundness]
   \item we do not however have the converse [completeness]
\end{itemize}

[Add 2 or 3 examples to show how it works and explain why we have the different steps]

\subsection{Soundness}
Soundness of the algorithm follows from the fact that the resulting clauses are obtained by directly instantiating universally quantified type variable of the initial clauses.

Then we rely upon the soundness of mangling [encoding mono and poly ty].

\subsection{Incompleteness}

This algorithm is incomplete, this is because not all possible instantiations of the type variables are generated.
Consider the following problem [note that we need a consistent notation for type arguments (notably to distinguish them from normal arguments) also this might not be an issue but they are terms not literals ]: 
\begin{align*}
   P = \{ & \forall \alpha. f \langle \alpha \rightarrow int \rangle \\
          & \forall \alpha. f \langle nat \rightarrow \alpha \rangle \\
          & \forall \alpha \beta. \lnot f \langle \alpha \rightarrow \beta \rangle \}
\end{align*}

The algorithm only matches against ground type arguments. This is the case because polymorphic type unification is undecidable (quick ref) and whilst interweaving phases of unification with the current algorithm would be theoretically possible, it would most likely not be of any practical use. Indeed, the current implementation is already very explosive and generating new substitutions with a costly unification procedure would most likely not result in any improvements.


The previous problem would therefore never instantiate a clause, despite the problem becoming trvially contradictory if the the last clause and any of the first two are instantiated.


Furthermore, the finite number of iterations of the algorithm consitutes an additional source of incompleteness in practice.
%find example that would require a list list list int to be solved but only allow such a type to be generated after 3 iterations

% target page number: 2-4
\section{Low level algorithm}
\label{sec:low-level-algorithm}

This section presents a more detailed description of the algorithm, it follows closely the implementation in Zipperposition.

The main departures from the high-level description of the algorithm are the following:
\begin{itemize}
   \item the substitutions are directly applied to the type arguments, instead of the clauses, this also dispenses us from having to re-extract the type arguments from the clauses at each iteration
   \item as a result the new clauses are generated only once all substitutions have been computed and iteratively applied to the various type arguments
   \item some operations are performed in a different order to avoid useless computations [which ones?]
   \item bounds are used at almost all steps of the process as the various enumerations are highly explosive and require bounds in order to be useful in a practical setting
   \item the various sets are implemented with maps
\end{itemize}

\begin{algorithm}[th]
\begin{algorithmic}[1]
\Function{Type arguments}{Literal: lit}

   \State let \(t_1\), \(t_2\) be the terms of \(lit\)
   \State recursively collect the function symbols and associated type arguments of \(t_1\) and \(t_2\) into a list of pairs

   \State \Comment Going a bit fast here, probably need to reformulate, but don't go into details, it's not important

   \State \Return the list of (function symbol, type argument) pairs

\EndFunction
\end{algorithmic}
\end{algorithm}


\begin{algorithm}[tbh]
\begin{algorithmic}[1]
\Function{Initialisation}{Problem: P}
   \ForAll{clause \(C \in P\)}
      \ForAll{literal \(lit \in C\)}
         \State let fun\_sym\_ty\_args = Type arguments(lit)
         \ForAll{(fun\_sym, ty\_args) in fun\_sym\_ty\_args}
         \If{\( \forall ty\_arg \in ty\_args. is\_ground(ty\_arg)\) }

            \State \(I(fun\_sym) = I(fun\_sym) \cup \{ty\_args\}\)

         \Else

            \State \(U(C, fun\_sym) = U(C, fun\_sym) \cup \{ty\_args\}\)

         \EndIf
         \EndFor
      \EndFor
   \EndFor

   \State \Return (I, U)

\EndFunction
\end{algorithmic}
\end{algorithm}

This function is very basic and simply gathers all the type arguments of the various function symbols of the problem and sorts them depending on whether they are ground or not.


\begin{algorithm}[tbh]
\begin{algorithmic}[1]
\Function{Iteration}{Problem: P, U, I, S}
   \ForAll {clause \(C \in P\)}
      \ForAll {\(fun\_sym, ty\_args \in U(C)\)}
      \ForAll {\( ty\_args' \in I(fun\_sym) \)}
            
            \State \(subst = match(ty\_args, ty\_arg')\)

            \State \Comment the match may fail in which case we simply continue with the loop

            \State \(S(C) = S(C) \cup \{subst\}\)
         \EndFor
      \EndFor
   
   \EndFor

   \ForAll {clause \(C \in P\)}
      \ForAll {\(subst \in S(C)\)}
         \ForAll {\(fun\_symbol, ty\_args \in U(C)\)}
            \State \(ty\_args' = apply(subst, ty\_args)\)

            \If {\( \forall ty\_arg \in ty\_args'. is\_ground(ty\_arg)\) }

               \State \(I(fun\_sym) = I(fun\_sym) \cup \{ty\_args'\}\)
            \Else

               \State \(U(C, fun\_sym) = U(C, fun\_sym) \cup \{ty\_args'\}\)
            \EndIf

         \EndFor
      \EndFor
   \EndFor

   \State \Return (I, U, S)

\EndFunction
\end{algorithmic}
\end{algorithm}
In the Zipperposition implemntation this iteration procedure is limited by several bounds. The number of new type arguments that can be generated for each function symbol is limited by user-defined bounds. These bounds can be defined proportionally. Additionally, the number of new type arguments can be limited at the clause level. In both cases, the bounds for the new ground type arguments and non-ground type arguments can be set independently.

A consequences of these bounds is that in order to avoid unecessary computations, the program will iteratively apply substitutions until either the ground or non-ground bound is reached. Then substitutions can be discarded depending on whether they instantiate a given set of type arguments or not.

Additionally, there are bounds for the number of substitutions that are generated.

\begin{algorithm}[tbh]
\begin{algorithmic}[1]
\Function{Instantiate clause}{Clause: C, Substitutions S}

   \If{\(C\) is ground}

      \State \Return \(\{C\}\)

   \Else

      \State let \(v\) be a type variable in \(C\)
      \State let new\_clauses be an empty set of clauses

      \ForAll{subst \(\in S(C)\)}

         \If{subst instantiates \(v\)}

            \State \(C' = apply(subst, C)\)

            \State let v\_clauses = Instantiate clause(C', S)

         \EndIf

         \State \( new\_clauses = new\_clauses \cup v\_clauses\)

      \EndFor

      \State \Return new\_clauses
   \EndIf

\EndFunction
\end{algorithmic}
\end{algorithm}
This function differs from the implementation in a significant way, indeed, through experimentation it was found that the generated clauses were far more likely to be solved by E if the substitutions that are combined to instantiate the clause were derived from the same iteration. In practice, we therefore store the iteration at which each substition is generated and at the instantiation phase only apply substitutions from the same iteration.


What the previous function does is essentially going through the tree of possible substitutions that instantiate the clause and collecting all possible instantiations. This is extremely explosive, and a hard cap on the number of clauses we want to generate is almost indispensable for any implementation.

\begin{algorithm}[tbh]
\begin{algorithmic}[1]
\Function{Generate clauses}{Problem: P, Substitutions: S}

\ForAll {clause \(C \in P\)}
   \State let new\_clauses = Instantiate clause(C, S)
   \State \(P = P \cup new\_clauses\)
\EndFor

\State \(P' = \emptyset \)

\ForAll {clause \(C \in P\)}
   \If{\(C\) is ground}
      \State \(P' = P' \cup \{mangle(C)\}\)
   \EndIf
\EndFor

\State \Return \(P'\)

\EndFunction
\end{algorithmic}
\end{algorithm}

Implementation in Zipperposition.

% target page number: 2-4
\section{Evaluation}
\label{sec:evaluation}

% Provide complete list of problems explain which ones were excluded and why

Compare the following:
    \begin{itemize}
        \item E on original portfolio mode
        \item E with original portfolio mode and monomorphisation
        \item adjusted portforlio mode 

        \item when E is called
        \item using zipp only as monomorphiser (passing the result to E, Vampire, Satallax, Leo-III)

        \item test different bounds
    \end{itemize}

\break


We ignore the following bounds: ty var limit per clause, subst per ty var, limiting the new type args per clause

\subsection{Parameter optimisation}

Considering the total number of parameters, it would be impractical to exhaustively test all option combinations. Consequently, options have been grouped together into sets of 2 or 3 options that will be more thouroughly tested. Two options will be part of the same group if it is deemed likely that a change in one of the options will significantly impact how much a change in the second option will affect the overall results.

The first two groups of options are natural as they affect how many new monomorphic and polymorphic type arguments are generated respectively for each function symbol at each iteration. These options are closely linked and must be set together if they are to be relevant.

During an iteration, for each function symbol (for each clause in the polymorphic case), the number of newly generated type arguments is determined by this formula:
\( \min(\text{cap}, \max(\text{floor}, \text{multiplier} \times \text{number of type arguments})) \)

The multiplier allows for a controlled increase of the number of type arguments, relative to the size of the problem whilst the floor prevents some function symbols seeing no new type arguments if the multiplier is too low.
The cap acts as a final limit in the case the explosion is too fast despite the multiplier bound.

\begin{table}[th]
\caption{Evaluation of bounds for monomorphic type argument generation}
\centering\begin{tabular}{@{}l*{9}{>{\centering\arraybackslash}p{1.1em}}@{}}
   \toprule
   & &&& \multicolumn{3}{c}{floor} \\
   & \multicolumn{3}{c}{1} & \multicolumn{3}{c}{3} & \multicolumn{3}{c}{9}\\
   \cmidrule(l){2-10}
   & &&& \multicolumn{3}{c}{multiplier} \\
   \multirow{1}{2em}{cap} & 0 & 1 & 2 & 0 & 1 & 2 & 0 & 1 & 2\\
    \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(l){8-10}
    25   & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    50   & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    100  & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    \(\infty\) & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    \bottomrule
\end{tabular}
\end{table}

\bigskip
\bigskip

If the floor and multiplier are set to 0 and 0, no new polymorphic type arguments will be generated, as a result the cap will be irrelevant.

\begin{table}[th]
\caption{Evaluation of bounds for polymorphic type argument generation}
\centering\begin{tabular}{@{}l*{9}{>{\centering\arraybackslash}p{1.1em}}@{}}
   \toprule
   & &&& \multicolumn{3}{c}{floor} \\
   & \multicolumn{3}{c}{0} & \multicolumn{3}{c}{3} & \multicolumn{3}{c}{6}\\
   \cmidrule(l){2-10}
   & &&& \multicolumn{3}{c}{multiplier} \\
    \multirow{1}{2em}{cap} & 0 & 0.5 & 1 & 0 & 0.5 & 1 & 0 & 0.5 & 1 \\
    \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(l){8-10}
    10 &   & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    20 &   & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    40 &   & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    \(\infty\) & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    \bottomrule
\end{tabular}
\end{table}

\begin{table}[th]
\caption{Evaluation of bounds for substitution generation}
\centering\begin{tabular}{@{}l*{4}{>{\centering\arraybackslash}p{1em}}@{}}
   \toprule
   & \multicolumn{4}{c}{mono subst}\\
   \multirow{1}{4em}{subst cap} & 2 & 5 & 7 & 10\\
   \midrule
   50   & 0 & 0 & 0 & 0\\
   100  & 0 & 0 & 0 & 0\\
   500  & 0 & 0 & 0 & 0\\
   \(\infty\) & 0 & 0 & 0 & 0\\
   
   \bottomrule
\end{tabular}
\end{table}

This table groups two bounds that directly affect the number of clauses generated, a first one that allows at most a number of new clauses equal to ``multiplier" times the original number of clauses. And the cap that gives an absolute bound over the total number of clauses generated.

The choice to also include e-timeout in this table is due to the fact that E being successful in solving a monomorphised problem in a certain time limit has been observed to be highly dependent on the number of clauses it is given.

\begin{table}[th]
\caption{Evaluation of bounds directly related to the size of the output problem}
\centering\begin{tabular}{@{}l*{9}{>{\centering\arraybackslash}p{1.1em}}@{}}
   \toprule
   & &&& \multicolumn{3}{c}{cap} \\
   & \multicolumn{3}{c}{500} & \multicolumn{3}{c}{2000} & \multicolumn{3}{c}{\(\infty\)}\\
   \cmidrule(l){2-10}
   & &&& \multicolumn{3}{c}{multiplier} \\
    \multirow{1}{5.4em}{E timeout (s)} & 1 & 2 & 3 & 1 & 2 & 3 & 1 & 2 & 3 \\
    \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(l){8-10}
    2   & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    5   & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    10  & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    \(\infty\)& 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    \bottomrule
\end{tabular}
\end{table}
The next table gathers the bounds linked to substitutions generation, the first one limits the number of monomorphising substitutions that are generated per clause during the final phase. The second one gives an absolute cap on the overall number of substitutions generated at each clause during the type argument generation.


This last table attempts to determine which are the best bounds related to the depth at which the monomorphisation problem is run as well as how long zipperposition is allowed to run before monomorphisation (and the subsequent call of E) is ran.

\begin{table}[th]
% not super happy with caption
\caption{Evaluation of parameters indirectly related to the size of the output problem}
\centering\begin{tabular}{@{}l*{4}{>{\centering\arraybackslash}p{1em}}@{}}
   \toprule
   & \multicolumn{4}{c}{loop nb} \\
   \multirow{1}{4.5em}{E call step} & 2 & 3 & 4 & 5\\
   \midrule
   0 & 0 & 0 & 0 & 0\\
   15 & 0 & 0 & 0 & 0\\
   45 & 0 & 0 & 0 & 0\\
   90 & 0 & 0 & 0 & 0\\
   
   \bottomrule
\end{tabular}
\end{table}



\subsection{Monomorphisation as preprocessor}

\begin{table}[ht]
\caption{Evaluation of native polymorphism vs.\ monomorphisation}
\centering\begin{tabular}{@{}lccc@{}}
   \toprule
   & Native & Mono & Union \\
   \midrule
   E  &   & 0 & 0 \\
   Leo-III & 0 & 0 & 0 \\
   Satallax &  & 0 & 0 \\
   Vampire & 0 & 0 & 0 \\
   Zipperposition & 0 & 0 & 0 \\[1.5\jot]
   Total & 0 & 0 & 0 \\
   \bottomrule
\end{tabular}
\end{table}

\subsection{E as Zipperposition backend}

\begin{table}[ht]
\caption{Evaluation of Zipperposition without E vs. with E}
\centering\begin{tabular}{@{}lccc@{}}
   \toprule
   & without E & with E & Union \\
   \midrule
   Zipperposition & 0 & 0 & 0 \\
   \bottomrule
\end{tabular}
\end{table}

\break

% target page number: 0.5-1
\section{Related work}
\label{sec:related-work}

% target page number: 0.5-1
\section{Conclusion}
\label{sec:conclusion}

% summary 

% Future avenues
\bibliography{citations}

\end{document}
