\documentclass[]{ceurart}

\usepackage{soul}
\usepackage{caption}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{array}
\usepackage{multirow}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsmath}

\lstset{breaklines=true}

\newcommand\ty[1]{\textsf{#1}}
\newcommand\sym[1]{\textsf{#1}}
\newcommand\var[1]{\mathit{#1}}

\newtheorem{definition}{Definition}

\begin{document}

\copyrightyear{2024}
\copyrightclause{Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0)}

\conference{Submitted draft} % TODO update

% TODO, at the end check the following
%   - formulas -> formulae
%   - clause -> formula
%   - monomorphization -> monomorphisation
%   - higer order -> higer-order
%   - TH1 with ugly 1
%   - correctly spelled TPTP, Leo-III, Zipperposition, E, Vampire
%   - use of formula vs clause
%   - polymorphic and monomorphic
%   - title of bits of the algorithms
%   - eg or ex -> e.g.

\title{Iterative Monomorphisation}

\author[1,2]{Tanguy Bozec}[%
email=tanguy.bozec@ens-paris-saclay.fr,
]
\author[2]{Jasmin Blanchette}[%
email=jasmin.blanchette@ifi.lmu.de,
%url=https://www.tcs.ifi.lmu.de/mitarbeiter/jasmin-blanchette_de.html,
]
\address[1]{ENS Paris-Saclay, Université Paris-Saclay, France}
\address[2]{Institute of Informatics, Ludwig-Maximilians-Universität München, Germany}


\begin{abstract}
Monomorphisation makes it possible to extend monomorphic provers to support polymorphic logics. We propose an iterative approach, which is necessarily incomplete but which works well in practice. It is implemented in the Zipperposition prover, where it can be used to translate away polymorphism before invoking a monomorphic prover. Our empirical evaluation reveals that iterative monomorphisation is a viable alternative to native polymorphism.
\end{abstract}

\begin{keywords}
   Polymorphism\sep
   monomorphism\sep
   automatic theorem proving
\end{keywords}

\maketitle

% target page number: 2-2.5
\section{Introduction}

One of the main applications of automatic theorem provers is to provide automation to users of proof assistants. Many proof assistants, such as HOL4 \cite{xxx}, HOL Light \cite{xxx} and Isabelle/HOL \cite{xxx}, support rank-1 polymorphism, where type quantification is allowed only at the top level of a formula. On the other hand, many provers operate only on monomorphic logics. One approach to close this gap is to extend provers to natively support polymorphism, as has been done for Vampire \cite{vamp}. This, however, entails a lot of work that needs to be redone for every prover.

  % apps are rank-1 polymorphic, but provers are mono
  % there's a gap
  % one solution is to extend the provers to support polymorphism;
  % this was done e.g. for Vampire \cite{xxx}
  % but this is a lot of work, and needs to be done for every prover

The alternative is to translate polymorphic problems to monomorphic problems. One approach is to encode polymorphism using guards or tags \cite{mono-trans} in a monomorphic logic. Another is to encode polymorphism using {iterative monomorphisation}, as introduced by B\"ohme \cite[Section 2.2.1]{sb-phd}. This method relies on heuristically instantiating the formulae's type variables with concrete types.

 % * the alternative is to translate polymorphic problems to monomorphic problems
 %   * two main approaches:
 %     * complete encoding of polymorphism, using guards or tags,
 %       as in Blanchette et al.~\cite{mono-trans} and Bobot et al.~\cite{expr-poly-types}
 %     * incomplete encoding of polymorphism based on iterative monomorphisation
 %       such as introduced by Böhme \cite[Section 2.2.1]{sb-phd}, whereby type
 %       variables are heuristically instantiated by concrete types

By a type version of the compactness theorem, we have that in first order logic, given a polymorphic formula \(\varphi\), there exist an equisatisfiable finite set of monomorphic instances of \(\varphi\). However, such a set cannot be computed \cite[Theorem 1]{expr-poly-types}. As a result, any monomorphisation method based on instantiation of type variables is
bound to be incomplete.

  %* By a type version of compactness, if a set of polymorphic
  %  formulae is unsatisfiable, then there exists a finite set of monomorphic
  %  instances of the formulae that is also unsatisfiable. However, this
  %  finite set cannot be computed \cite[Section 2, Theorem 1]{expr-poly-types}; hence any approach based on
  %  instantiation of type variables is incomplete.


B\"ohme's iterative approach is implemented as part of the SMT (satisfiability modulo theories) integration \cite[Chapter 2]{sb-phd} in Isabelle/HOL. This implementation is also used by Sledgehammer \cite{judgement, hammer} to interface with superposition based automatic theorem provers. However, it is is only documented as a single subsection in his PhD thesis \cite[Section 2.2.1]{sb-phd}.

In this paper, we present an algorithm based on his description and implementation (Section~\ref{sec:high-level-algorithm}). We also provide a more detailed description to help future implementers. In addition, this description shows some of the ways in which an implementation can be made more efficient and avoid explosions (Section~\ref{sec:low-level-algorithm}).

% * Böhme's iterative approach is implemented as part of the SMT integration \cite[Chapter 2]{sb-phd}
%     in Isabelle/HOL. This implementation is also used by
%    Sledgehammer \cite{judgement, hammer} to interface automatic theorem provers based on
%    superposition.
%
%  * The main drawback of Böhme's approach is that it is only briefly documented,
%    as a single paragraph in his PhD thesis \cite[Section 2.2.1]{sb-phd}
%  * In this paper, we present an algorithm inspired by his description and
%    implementation (Section~\ref{sec:high-level-algorithm}).
%    * we aimed at mathematical precision, so that people who want to reimplement
%      the algorithm can follow this blueprint
%    * we also show how to bound the algorithm to avoid explosions in practice
%      (Section~\ref{sec:low-level-algorithm}).
%      ``the explosive nature of the procedures requires various bounds which
%      limit the number of type arguments and clauses that are generated.''


The algorithm works as follows. We assume problems to be sets of formulae. All symbols of the problem are collected, and the polymorphic instances of symbols are matched against the monomorphic ones. This yields new instances of symbols, both polymorphic and monomorphic. The process is then iterated
a number of times, making use of the newly generated instances.

%These iterations are required to account for $n$-ary type constructors.
Consider the unary type constructor \ty{list}. If a formula contains $\ty{list}(\alpha)$, where $\alpha$ is a type variable, it may be possible to generate the types $\ty{list}(\ty{int})$, $\ty{list}(\ty{list}(\ty{int}))$, etc. However, because new types emerge through matching, $\ty{list}(\ty{list}(\ty{int}))$ can be obtained only once the $\ty{list}(\ty{int})$ instance has already been generated. This example also makes it clear that the set of instances that can be generated is infinite.

To keep the number of generated formulae finite, we limit the number of iterations. After the iterations are completed, the new monomorphic symbol instances are used to instantiate the polymorphic symbols in the problem's formulae, generating new monomorphic formulae. Finally, because monomorphic provers generally do not support $n$-ary type constructors, types must be `mangled'; for example, the compound type $\ty{list}(\ty{int})$ could be mangled to the constant $\ty{list\_int}$.

We implemented iterative monormorphisation in Zipperpostion \cite{zipp}, a higher order prover written in OCaml. Although Zipperposition is polymorphic, it uses the monomorphic prover E \cite{e} as a backend. This means that E can now be used with polymorphic problems. Moreover, our implementation of the algorithm in Zipperposition can be used as a preprocessor to interface with other stand-alone provers.

Our empirical evaluation on TPTP \cite{tptp} problems attempts to answer three questions (Section~\ref{sec:evaluation}):
%
\begin{enumerate}
\item Is the new Zipperposition with the E backend more successful on polymorphic problems than Zipperposition without backend?

\item How competitive are monomorphic provers on monomorphised polymorphic problems?

\item Is iterative monomorphisation more effective than the native polymorphism implemented in polymorphic provers?
\end{enumerate}

Our findings are as follows:
%
\begin{enumerate}
\item Zipperposition benefits substantially from the E backend.

\item E with monomorphisation comes close second to the polymorphic prover Vampire.

\item For Leo-III \cite{leo-iii} and Vampire \cite{vamp}, we find that monomorphisation is indeed more effective than native polymorphism.
\end{enumerate}

% target page number: 0.5-1.5
\section{Background}
\label{sec:background}

% TH1 ugly for height of 1 reasons, can be fixed with case feature
%The base logic of the implementation will be TPTP's TH1 \cite{th1}, a higher-order logic with rank-1 polymorphism. We will assume the initial problem to be a set of TH1 formulae. The choice of this logic is motivated by the widespread support it enjoys among higher-order provers. However, iterative monomorphisation only 
The implementation require problems to be based on TPTP's TH1 (or TF1) logic \cite{th1} because it is the a widespread standard among provers for higher-order rank-1 polymorphic logics. However, the iterative monomorphisation algorithm relies only on assumptions regarding type construction and use in the problem. Consequently, the precise forms of formulae, terms or literals are irrelevant.


\begin{definition}[Function symbol]
   A function symbol \(f\), has two arities which define the number of arguments and type arguments it takes. The type arguments are enumerated between angled brackets and the other arguments between parentheses, e.g.: \(f\langle \tau_1, ..., \tau_n\rangle (x_1, ..., x_m)\)

   In a well formed problem, the number of type arguments to which a function symbol is applied is assumed to correspond to the relevant arity. In other words, we expect all function symbols of type argument arity \(n\) to have \(n\) type arguments.
\end{definition}

% TODO: WARNING - heavily inspired from sections 2.2-2.3 of the th1 paper
\begin{definition}[Type]
A type \(\tau\) can be:
\begin{itemize}
   \item a constant, e.g.: \(\ty{int}\)
   \item a type variable universally quantified at the formula level, e.g.: \(\alpha\)
   \item built from an \(n\)-ary type constructor applied to \(n\) types, e.g.: \(\ty{map}(\ty{int}, \ty{list}(\alpha))\)
\end{itemize}

\end{definition}

\begin{definition}[Monomorphic, Polymorphic types]
A type is monomorphic if it contains no type variables, otherwise it is polymorphic.
\end{definition}


% target page number: 2-4
\section{High level algorithm}
\label{sec:high-level-algorithm}

We consider a problem \(P\) to be a set of formulae \(F_1, \dots, F_n\).

\subsection{Initialisation step}
The initial phase of the algorithm consists in gathering the maps \(M\) and \(P\), defined as follows:
\begin{itemize}
    \item Given a type \(n\)-ary function symbol \(f \in P\), we define \(M(f)\), the set of all \(n\)-uples of monomorphic type arguments passed to \(f\) in \(P\).

    For example, if \(f\langle \tau_1, \tau_2, \tau_3\rangle\) occurs in \(P\) (with \(\tau_1, \tau_2\) and \( \tau_3\) monomorphic) then, \((\tau_1, \tau_2, \tau_3) \in M(f) \).

 \item Given a formula \(F \in P\) and a type \(n\)-ary function symbol \(f \in F\), we define \(P(C, f)\), the set \(n\)-uples of polymorphic type arguments passed to \(f\) in \(F\). 

    It is important to paramatrise \(P\) with the formula of \(f\) because type variables are quantified at the formula level. This is not necessary for \(M\) because its types do not contain type variables.

\end{itemize}

\subsection{Iteration}
A single iteration of the algorithm will do the following:

\begin{itemize}
   \item For each clause \(C_i \in P\) and each function symbol \(f \in C_i\), we consider the uninstantiated type arguments \(U(C_i, f)\) these type type arguments are matched against all ground type arguments of \(I(f)\)

      [We have an issue: we don't define matching, don't think we need to go into a lot of detail]

      Each such matching may return a substitution which instantiates one or more type variables of \(C_i\) (that would also be in the type arguments of \(f\))

   \item Once all such substitutions have been computed. We apply each substitutions to the clause they were generated from. These newly created clauses are added to the problem and the newly generated type arguments (both ground and uninstantiated) will be added to \(U\) and \(I\) to be used for the next iteration.
\end{itemize}

After a set number of iterations, we take all clauses with no type variables and mangle their types [see relevant paper]. The resulting set of clauses forms a problem \(P'\) with the following properties:
\begin{itemize}
   \item \(P'\) is monomorphic [rewrite that as saying \(P'\) is in the monomorphic fragment of the original logic]
   \item if a contradiction can be derived from \(P'\), then a contradiction can be derived from \(P\) [soundness]
   \item we do not however have the converse [completeness]
\end{itemize}


This algorithm is clearly sound as the newly derived clauses are instances of the initial clauses where universally quantified types have been replaced by concrete types.

% target page number: 2-4
\section{Low level algorithm}
\label{sec:low-level-algorithm}

This section presents a more detailed description of the algorithm, it follows closely the implementation in Zipperposition.

The main departures from the high-level description of the algorithm are the following:
\begin{itemize}
   \item the substitutions are directly applied to the type arguments, instead of the clauses, this also dispenses us from having to re-extract the type arguments from the clauses at each iteration
   \item as a result the new clauses are generated only once all substitutions have been computed and iteratively applied to the various type arguments
   \item some operations are performed in a different order to avoid useless computations [which ones?]
   \item bounds are used at almost all steps of the process as the various enumerations are highly explosive and require bounds in order to be useful in a practical setting
   \item the various sets are implemented with maps
\end{itemize}

\begin{algorithm}[th]
\begin{algorithmic}[1]
\Function{Type arguments}{Literal: lit}

   \State let \(t_1\), \(t_2\) be the terms of \(lit\)
   \State recursively collect the function symbols and associated type arguments of \(t_1\) and \(t_2\) into a list of pairs

   \State \Comment Going a bit fast here, probably need to reformulate, but don't go into details, it's not important

   \State \Return the list of (function symbol, type argument) pairs

\EndFunction
\end{algorithmic}
\end{algorithm}


\begin{algorithm}[tbh]
\begin{algorithmic}[1]
\Function{Initialisation}{Problem: P}
   \ForAll{clause \(C \in P\)}
      \ForAll{literal \(lit \in C\)}
         \State let fun\_sym\_ty\_args = Type arguments(lit)
         \ForAll{(fun\_sym, ty\_args) in fun\_sym\_ty\_args}
         \If{\( \forall ty\_arg \in ty\_args. is\_ground(ty\_arg)\) }

            \State \(I(fun\_sym) = I(fun\_sym) \cup \{ty\_args\}\)

         \Else

            \State \(U(C, fun\_sym) = U(C, fun\_sym) \cup \{ty\_args\}\)

         \EndIf
         \EndFor
      \EndFor
   \EndFor

   \State \Return (I, U)

\EndFunction
\end{algorithmic}
\end{algorithm}

This function is very basic and simply gathers all the type arguments of the various function symbols of the problem and sorts them depending on whether they are ground or not.


\begin{algorithm}[tbh]
\begin{algorithmic}[1]
\Function{Iteration}{Problem: P, U, I, S}
   \ForAll {clause \(C \in P\)}
      \ForAll {\(fun\_sym, ty\_args \in U(C)\)}
      \ForAll {\( ty\_args' \in I(fun\_sym) \)}
            
            \State \(subst = match(ty\_args, ty\_arg')\)

            \State \Comment the match may fail in which case we simply continue with the loop

            \State \(S(C) = S(C) \cup \{subst\}\)
         \EndFor
      \EndFor
   
   \EndFor

   \ForAll {clause \(C \in P\)}
      \ForAll {\(subst \in S(C)\)}
         \ForAll {\(fun\_symbol, ty\_args \in U(C)\)}
            \State \(ty\_args' = apply(subst, ty\_args)\)

            \If {\( \forall ty\_arg \in ty\_args'. is\_ground(ty\_arg)\) }

               \State \(I(fun\_sym) = I(fun\_sym) \cup \{ty\_args'\}\)
            \Else

               \State \(U(C, fun\_sym) = U(C, fun\_sym) \cup \{ty\_args'\}\)
            \EndIf

         \EndFor
      \EndFor
   \EndFor

   \State \Return (I, U, S)

\EndFunction
\end{algorithmic}
\end{algorithm}
In the Zipperposition implemntation this iteration procedure is limited by several bounds. The number of new type arguments that can be generated for each function symbol is limited by user-defined bounds. These bounds can be defined proportionally. Additionally, the number of new type arguments can be limited at the clause level. In both cases, the bounds for the new ground type arguments and non-ground type arguments can be set independently.

A consequences of these bounds is that in order to avoid unecessary computations, the program will iteratively apply substitutions until either the ground or non-ground bound is reached. Then substitutions can be discarded depending on whether they instantiate a given set of type arguments or not.

Additionally, there are bounds for the number of substitutions that are generated.

\begin{algorithm}[tbh]
\begin{algorithmic}[1]
\Function{Instantiate clause}{Clause: C, Substitutions S}

   \If{\(C\) is ground}

      \State \Return \(\{C\}\)

   \Else

      \State let \(v\) be a type variable in \(C\)
      \State let new\_clauses be an empty set of clauses

      \ForAll{subst \(\in S(C)\)}

         \If{subst instantiates \(v\)}

            \State \(C' = apply(subst, C)\)

            \State let v\_clauses = Instantiate clause(C', S)

         \EndIf

         \State \( new\_clauses = new\_clauses \cup v\_clauses\)

      \EndFor

      \State \Return new\_clauses
   \EndIf

\EndFunction
\end{algorithmic}
\end{algorithm}
This function differs from the implementation in a significant way, indeed, through experimentation it was found that the generated clauses were far more likely to be solved by E if the substitutions that are combined to instantiate the clause were derived from the same iteration. In practice, we therefore store the iteration at which each substition is generated and at the instantiation phase only apply substitutions from the same iteration.


What the previous function does is essentially going through the tree of possible substitutions that instantiate the clause and collecting all possible instantiations. This is extremely explosive, and a hard cap on the number of clauses we want to generate is almost indispensable for any implementation.

\begin{algorithm}[tbh]
\begin{algorithmic}[1]
\Function{Generate clauses}{Problem: P, Substitutions: S}

\ForAll {clause \(C \in P\)}
   \State let new\_clauses = Instantiate clause(C, S)
   \State \(P = P \cup new\_clauses\)
\EndFor

\State \(P' = \emptyset \)

\ForAll {clause \(C \in P\)}
   \If{\(C\) is ground}
      \State \(P' = P' \cup \{mangle(C)\}\)
   \EndIf
\EndFor

\State \Return \(P'\)

\EndFunction
\end{algorithmic}
\end{algorithm}

Implementation in Zipperposition.

% target page number: 2-4
\section{Evaluation}
\label{sec:evaluation}

% Provide complete list of problems explain which ones were excluded and why
% We ignore the following bounds: ty var limit per clause, subst per ty var, limiting the new type args per clause

\subsection{Parameter optimisation}

Considering the total number of parameters, it would be impractical to exhaustively test all option combinations. Consequently, options have been grouped together into sets of 2 or 3 options that will be more thouroughly tested. Two options will be part of the same group if it is deemed likely that a change in one of the options will significantly impact how much a change in the second option will affect the overall results.

The first two groups of options are natural as they affect how many new monomorphic and polymorphic type arguments are generated respectively for each function symbol at each iteration. These options are closely linked and must be set together if they are to be relevant.

During an iteration, for each function symbol (for each clause in the polymorphic case), the number of newly generated type arguments is determined by this formula:
\( \min(\text{cap}, \max(\text{floor}, \text{multiplier} \times \text{number of type arguments})) \)

The multiplier allows for a controlled increase of the number of type arguments, relative to the size of the problem whilst the floor prevents some function symbols seeing no new type arguments if the multiplier is too low.
The cap acts as a final limit in the case the explosion is too fast despite the multiplier bound.

\begin{table}[th]
\caption{Evaluation of bounds for monomorphic type argument generation}
\centering\begin{tabular}{@{}l*{9}{>{\centering\arraybackslash}p{1.1em}}@{}}
   \toprule
   & &&& \multicolumn{3}{c}{floor} \\
   & \multicolumn{3}{c}{1} & \multicolumn{3}{c}{3} & \multicolumn{3}{c}{9}\\
   \cmidrule(l){2-10}
   & &&& \multicolumn{3}{c}{multiplier} \\
   \multirow{1}{2em}{cap} & 0 & 1 & 2 & 0 & 1 & 2 & 0 & 1 & 2\\
    \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(l){8-10}
    25   & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    50   & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    100  & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    \(\infty\) & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    \bottomrule
\end{tabular}
\end{table}

\bigskip
\bigskip

If the floor and multiplier are set to 0 and 0, no new polymorphic type arguments will be generated, as a result the cap will be irrelevant.

\begin{table}[th]
\caption{Evaluation of bounds for polymorphic type argument generation}
\centering\begin{tabular}{@{}l*{9}{>{\centering\arraybackslash}p{1.1em}}@{}}
   \toprule
   & &&& \multicolumn{3}{c}{floor} \\
   & \multicolumn{3}{c}{0} & \multicolumn{3}{c}{3} & \multicolumn{3}{c}{6}\\
   \cmidrule(l){2-10}
   & &&& \multicolumn{3}{c}{multiplier} \\
    \multirow{1}{2em}{cap} & 0 & 0.5 & 1 & 0 & 0.5 & 1 & 0 & 0.5 & 1 \\
    \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(l){8-10}
    10 &   & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    20 &   & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    40 &   & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    \(\infty\) & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    \bottomrule
\end{tabular}
\end{table}

\begin{table}[th]
\caption{Evaluation of bounds for substitution generation}
\centering\begin{tabular}{@{}l*{4}{>{\centering\arraybackslash}p{1em}}@{}}
   \toprule
   & \multicolumn{4}{c}{mono subst}\\
   \multirow{1}{4em}{subst cap} & 2 & 5 & 7 & 10\\
   \midrule
   50   & 0 & 0 & 0 & 0\\
   100  & 0 & 0 & 0 & 0\\
   500  & 0 & 0 & 0 & 0\\
   \(\infty\) & 0 & 0 & 0 & 0\\
   
   \bottomrule
\end{tabular}
\end{table}

This table groups two bounds that directly affect the number of clauses generated, a first one that allows at most a number of new clauses equal to `multiplier' times the original number of clauses. And the cap that gives an absolute bound over the total number of clauses generated.

The choice to also include e-timeout in this table is due to the fact that E being successful in solving a monomorphised problem in a certain time limit has been observed to be highly dependent on the number of clauses it is given.

\begin{table}[th]
\caption{Evaluation of bounds directly related to the size of the output problem}
\centering\begin{tabular}{@{}l*{9}{>{\centering\arraybackslash}p{1.1em}}@{}}
   \toprule
   & &&& \multicolumn{3}{c}{cap} \\
   & \multicolumn{3}{c}{500} & \multicolumn{3}{c}{2000} & \multicolumn{3}{c}{\(\infty\)}\\
   \cmidrule(l){2-10}
   & &&& \multicolumn{3}{c}{multiplier} \\
    \multirow{1}{5.4em}{E timeout (s)} & 1 & 2 & 3 & 1 & 2 & 3 & 1 & 2 & 3 \\
    \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(l){8-10}
    2   & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    5   & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    10  & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    \(\infty\)& 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    \bottomrule
\end{tabular}
\end{table}
The next table gathers the bounds linked to substitutions generation, the first one limits the number of monomorphising substitutions that are generated per clause during the final phase. The second one gives an absolute cap on the overall number of substitutions generated at each clause during the type argument generation.


This last table attempts to determine which are the best bounds related to the depth at which the monomorphisation problem is run as well as how long zipperposition is allowed to run before monomorphisation (and the subsequent call of E) is ran.

\begin{table}[th]
% not super happy with caption
\caption{Evaluation of parameters indirectly related to the size of the output problem}
\centering\begin{tabular}{@{}l*{4}{>{\centering\arraybackslash}p{1em}}@{}}
   \toprule
   & \multicolumn{4}{c}{loop nb} \\
   \multirow{1}{4.5em}{E call step} & 2 & 3 & 4 & 5\\
   \midrule
   0 & 0 & 0 & 0 & 0\\
   15 & 0 & 0 & 0 & 0\\
   45 & 0 & 0 & 0 & 0\\
   90 & 0 & 0 & 0 & 0\\
   
   \bottomrule
\end{tabular}
\end{table}



\subsection{Monomorphisation as preprocessor}

\begin{table}[ht]
\caption{Evaluation of native polymorphism vs.\ monomorphisation}
\centering\begin{tabular}{@{}lccc@{}}
   \toprule
   & Native & Mono & Union \\
   \midrule
   E  &   & 0 & 0 \\
   Leo-III & 0 & 0 & 0 \\
   Satallax &  & 0 & 0 \\
   Vampire & 0 & 0 & 0 \\
   Zipperposition & 0 & 0 & 0 \\[1.5\jot]
   Total & 0 & 0 & 0 \\
   \bottomrule
\end{tabular}
\end{table}

\subsection{E as Zipperposition backend}

\begin{table}[ht]
\caption{Evaluation of Zipperposition without E vs. with E}
\centering\begin{tabular}{@{}lccc@{}}
   \toprule
   & without E & with E & Union \\
   \midrule
   Zipperposition & 0 & 0 & 0 \\
   \bottomrule
\end{tabular}
\end{table}

\break

% target page number: 0.5-1
\section{Related work}
\label{sec:related-work}

% target page number: 0.5-1
\section{Conclusion}
\label{sec:conclusion}

% summary 

% Future avenues
\bibliography{citations}

\end{document}
