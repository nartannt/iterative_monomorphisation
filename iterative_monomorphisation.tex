\documentclass{article}

\usepackage{soul}
\usepackage{caption}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsmath}

\setlength{\parindent}{0pt}

\begin{document}

TODO:
   \begin{itemize}

      \item split the problems in two sets one for refining parameters and bounds, the other for overall evaluation
      \item read mangling paper in detail
      \item set up tables
      \item finish implementation (mostly there)
      \item test and evaluate implementation 
      \item (can wait) merge implementation (review code with active contributor(s) beforehand)
      \item find pretty inline setup for pseudo code
      \item (in order) write high/low level algo
      \item write implementation and evaluation
      \item write background
      \item write related work
      \item write conclusion
      \item write intro
      \item reread, spellcheck ...
    \end{itemize}

% target page number: 2-2.5
\section{Introduction}
\begin{itemize}
    \item context and motivation (automatic theorem proving, better performance)\\
       Automatic theorem provers have many applications, and are particularly useful as backends to proof assistants. Speed is one of the main limitations of the use of theorem provers in this setting. Indeed, a prover can often be expected to run for no more than a few seconds for a given instance of a problem.
    \item what we are trying to do (monomorphisation)\\
       This paper presents a method, first introduced in [sacha's thesis] [as far as i know], that allows for higher order polymorphic problems to be approximated monomorphically. This algorithm aims to acheive the following:
         \begin{itemize}
            \item allow for existing higher order monomorphic theorem provers to be extended [albeit very crudely] to higher order polymorphic languages
            \item improve the performance of existing higher order polymorphic theorem provers on polymorphic problems
         \end{itemize}

         It is therefore aimed at being practically useful, to this end it forgoes completeness, this is the main theoretical weakness of this algorithm [the real main theoretical weakness would be that it is unsound, but i've done my best to make sure it isn't]. [ref to example in the paper where we explain that?]. 

    \item basic idea of the algorithm

    A given problem is seen as a set of polymorphic clauses. Type variables of the problem are quantified at the clause level. The type arguments of the problem are collected and the uninstantiated type arguments are matched against those that contain no type variables. This yields new type arguments, both uninstantitated and ground, the process is then repeated making use of the newly derived type arguments.

    The new type arguments we have derived are used to create new clauses, without type variables. This can lead to the same clause being duplicated as one type variable may be instantiated in a number of different ways.
    A final step is required as the newly generated problem, whilst it does not contain type variables, still makes use of type constructors. Those can be transparently erased through the mangling method presented in [relevant paper].
    \item overview of the paper

       This paper will first give some general background and set the theoretical framework that will be considered. Then a high-level mathematical description of the algorithm will be presented. A more detailed low-level description of the algorithm corresponding to its actual implementation will follow.
       Finally the implementation and evaluation of the algorithm in zippeposition [the award winning higher order theorem prover] [introduce zipperposition before] will be discussed.

       [issue with intro: focuses too much on theoretical aspect bearly mentions zipp or implementation]
\end{itemize}

% target page number: 0.5-1.5
\section{Background}

[Start out by mentionning popular type encodings, such as tags and gards, explain the interest of this method]
Why we didn't implement the types / guards method: in part because it relies on inferences on types that are given in Isabelle but we would have had to do ourselves

\begin{itemize}
    \item Language, TF1 (might be TH1 or something else)
\end{itemize}

We want our starting logic to be:
\begin{itemize}
    \item higher order, we want lambdas, function symbols and the rest
    \item polymorphic, we want atomic types, type constructors and type variables
\end{itemize}
and our target logic to be the same thing without polymorphism make sure to point out that the mono logic is a subset of the poly logic


Definitions we need:
\begin{itemize}
   \item type arguments (probably part of logic)
   \item ground and uninstantiated types 
   \item vaguely define what matching is
   \item define what a term, literal and clause are
\end{itemize}
Need to define ground types: without type variables and uninstantiated types, with type variables

DO NOT use monomorphic to mean ground and polymorphic to mean uninstantiated, everything is polymorphic until we mangle, then everything is monomorphic

Unsure of which terms to use: ground and non-ground, or instantiated and uninstantiated or both


% target page number: 2-4
\section{High level algorithm}

We consider a problem \(P\) to be a set of clauses \(C_1, \dots, C_n\).

Algorithm :
\begin{itemize}
    \item Let \(F\) be the set of all function symbols in \(P\).
    \item Given a function symbol \(f \in F\), we define \(I(f)\), the set of all ground type arguments that are passed to \(f\) is instantiated in \(P\).

    For instance if \(f^{a \rightarrow b}\) occurs in \(P\) (with \(a\) and \(b\) ground types) then, \(a \rightarrow b \in I(f) \).
 \item Given a clause \(C \in P\) and a function symbol \(f \in C\), we define \(U(C, f)\), the set of uninstantiated type arguments passed to \(f\) in \(C\). 

    It is important to paramatrise \(U\) with the clause of \(f\) because type variables are quantified at the clause level. This is not necessary for \(I\) because its type arguments do not contain type variables. [seems a sufficient explanation, if not clear expand further]

\end{itemize}

A single iteration of the algorithm will do the following:
\begin{itemize}
   \item For each clause \(C_i \in P\) and each function symbol \(f \in C_i\), we consider the uninstantiated type arguments \(U(C_i, f)\) these type type arguments are matched against all ground type arguments of \(I(f)\)

      [We have an issue: we don't define matching, don't think we need to go into a lot of detail]

      Each such matching may return a substitution which instantiates one or more type variables of \(C_i\) (that would also be in the type arguments of \(f\))

   \item Once all such substiutions have been computed. We apply each substitutions to the clause they were generated from. These newly created clauses are added to the problem and the newly generated type arguments (both ground and uninstantiated) will be added to \(U\) and \(I\) to be used for the next iteration.
\end{itemize}

After a set number of iterations, we take all clauses with no type variables and mangle their types [see relevant paper]. The resulting set of clauses forms a problem \(P'\) with the following properties:
\begin{itemize}
   \item \(P'\) is monomorphic [rewrite that as saying \(P'\) is in the monomorphic fragment of the original logic]
   \item if a contradiction can be derived from \(P'\), then a contradiction can be derived from \(P\) [soundness]
   \item we do not however have the converse [completeness]
\end{itemize}

[Add 2 or 3 examples to show how it works and explain why we have the different steps]

\subsection{Soundness}
[this isn't an actual proof, more of a justification, mostly a joke in its present form]
Soundness of the algorithm follows from the fact that the resulting clauses are obtained by directly instantiating universally quantified type variable of the initial clauses. Then we rely upon the soundness of mangling [include reference].

\subsection{Multiple type arguments}
Comment that a function symbol with multiple type arguments will often see them instantiated one at a time per iteration. Conclude that it may be beneficial to remove clauses with function symbols with too many uninstantiated type arguments. Nuance that if the more there are such symbols, the more likely we are to generate substitutions that instantiate several type arguments at a time. Note that in practice setting a type variable limit changes nothing.
[more tests needed for showing that
[need to add a couple of tests to show that]
[maybe this section doesn't belong here, probably better in the low level section]

\subsection{Incompleteness}

This algorithm is incomplete, this is because not all possible instantiations of the type variables are generated.
Consider the following problem [note that we need a consistent notation for type arguments (notably to distinguish them from normal arguments) also this might not be an issue but they are terms not literals ]: 
\begin{align*}
   P = \{ & \forall \alpha. f \langle \alpha \rightarrow int \rangle \\
          & \forall \alpha. f \langle nat \rightarrow \alpha \rangle \\
          & \forall \alpha \beta. \lnot f \langle \alpha \rightarrow \beta \rangle \}
\end{align*}

The algorithm only matches against ground type arguments. This is the case because polymorphic type unification is undecidable (quick ref) and whilst interweaving phases of unification with the current algorithm would be theoretically possible, it would most likely not be of any practical use. Indeed, the current implementation is already very explosive and generating new substitutions with a costly unification procedure would most likely not result in any improvements.


The previous problem would therefore never instantiate a clause, despite the problem becoming trvially contradictory if the the last clause and any of the first two are instantiated.


Furthermore, the finite number of iterations of the algorithm consitutes an additional source of incompleteness in practice.
[find example that would require a list list list int to be solved but only allow such a type to be generated after 3 iterations]
% target page number: 2-4
\section{Low level algorithm}

This section presents a more detailed description of the algorithm, it follows closely the implementation in zipperposition.

The main departures from the high-level description of the algorithm are the following:
\begin{itemize}
   \item the substitutions are directly applied to the type arguments, instead of the clauses, this also dispenses us from having to re-extract the type arguments from the clauses at each iteration
   \item as a result the new clauses are generated only once all substitutions have been computed and iteratively applied to the various type arguments
   \item some operations are performed in a different order to avoid useless computations [which ones?]
   \item bounds are used at almost all steps of the process as the various enumerations are highly explosive and require bounds in order to be useful in a practical setting
   \item the various sets are implemented with maps
\end{itemize}

Precise algo description (pseudo code). Explicit loops, and bounds.

[note that this is all very ugly and will be made beautiful once the first draft (of the algorithm) is done]

\begin{algorithm}[tbh]
\begin{algorithmic}[1]
\Function{Type arguments}{Literal: lit}

   \State let \(t_1\), \(t_2\) be the terms of \(lit\)
   \State recursively collect the function symbols and associated type arguments of \(t_1\) and \(t_2\) into a list of pairs

   \State \Comment Going a bit fast here, probably need to reformulate, but don't go into details, it's not important

   \State \Return the list of (function symbol, type argument) pairs

\EndFunction
\end{algorithmic}
\end{algorithm}


\begin{algorithm}[tbh]
\begin{algorithmic}[1]
\Function{Initialisation}{Problem: P}
   \ForAll{clause \(C \in P\)}
      \ForAll{literal \(lit \in C\)}
         \State let fun\_sym\_ty\_args = Type arguments(lit)
         \ForAll{(fun\_sym, ty\_args) in fun\_sym\_ty\_args}
         \If{\( \forall ty\_arg \in ty\_args. is\_ground(ty\_arg)\) }

            \State \(I(fun\_sym) = I(fun\_sym) \cup \{ty\_args\}\)

         \Else

            \State \(U(C, fun\_sym) = U(C, fun\_sym) \cup \{ty\_args\}\)

         \EndIf
         \EndFor
      \EndFor
   \EndFor

   \State \Return (I, U)

\EndFunction
\end{algorithmic}
\end{algorithm}

This function is very basic and simply gathers all the type arguments of the various function symbols of the problem and sorts them depending on whether they are ground or not.


\begin{algorithm}[tbh]
\begin{algorithmic}[1]
\Function{Iteration}{Problem: P, U, I, S}
   \ForAll {clause \(C \in P\)}
      \ForAll {\(fun\_sym, ty\_args \in U(C)\)}
      \ForAll {\( ty\_args' \in I(fun\_sym) \)}
            
            \State \(subst = match(ty\_args, ty\_arg')\)

            \State \Comment the match may fail in which case we simply continue with the loop

            \State \(S(C) = S(C) \cup \{subst\}\)
         \EndFor
      \EndFor
   
   \EndFor

   \ForAll {clause \(C \in P\)}
      \ForAll {\(subst \in S(C)\)}
         \ForAll {\(fun\_symbol, ty\_args \in U(C)\)}
            \State \(ty\_args' = apply(subst, ty\_args)\)

            \If {\( \forall ty\_arg \in ty\_args'. is\_ground(ty\_arg)\) }

               \State \(I(fun\_sym) = I(fun\_sym) \cup \{ty\_args'\}\)
            \Else

               \State \(U(C, fun\_sym) = U(C, fun\_sym) \cup \{ty\_args'\}\)
            \EndIf

         \EndFor
      \EndFor
   \EndFor

   \State \Return (I, U, S)

\EndFunction
\end{algorithmic}
\end{algorithm}
In the zipperposition implemntation this iteration procedure is limited by several bounds. The number of new type arguments that can be generated for each function symbol is limited by user-defined bounds. These bounds can be defined proportionally. Additionally, the number of new type arguments can be limited at the clause level. In both cases, the bounds for the new ground type arguments and non-ground type arguments can be set independently.

A consequences of these bounds is that in order to avoid unecessary computations, the program will iteratively apply substitutions until either the ground or non-ground bound is reached. Then substitutions can be discarded depending on whether they instantiate a given set of type arguments or not.

Additionally, there are bounds for the number of substitutions that are generated.

[So far, after more or less blind experimentation, I've settled on severely limiting new non-ground type arguments because they used to balloon out of control, whilst the growth in ground type arguments is more manageable]
[It's also worth suggesting the possibility of a more sophisticated implementation that could allow for dynamic bounds]

\begin{algorithm}[tbh]
\begin{algorithmic}[1]
\Function{Instantiate clause}{Clause: C, Substitutions S}

   \If{\(C\) is ground}

      \State \Return \(\{C\}\)

   \Else

      \State let \(v\) be a type variable in \(C\)
      \State let new\_clauses be an empty set of clauses

      \ForAll{subst \(\in S(C)\)}

         \If{subst instantiates \(v\)}

            \State \(C' = apply(subst, C)\)

            \State let v\_clauses = Instantiate clause(C', S)

         \EndIf

         \State \( new\_clauses = new\_clauses \cup v\_clauses\)

      \EndFor

      \State \Return new\_clauses
   \EndIf

\EndFunction
\end{algorithmic}
\end{algorithm}
This function differs from the implementation in a significant way, indeed, through experimentation it was found that the generated clauses were far more likely to be solved by E if the substitutions that are combined to instantiate the clause were derived from the same iteration. In practice, we therefore store the iteration at which each substition is generated and at the instantiation phase only apply substitutions from the same iteration.

[need to make a handful of tests to check that]

What the previous function does is essentially going through the tree of possible substitutions that instantiate the clause and collecting all possible instantiations. This is extremely explosive, and a hard cap on the number of clauses we want to generate is almost indispensable for any implementation.

\begin{algorithm}[tbh]
\begin{algorithmic}[1]
\Function{Generate clauses}{Problem: P, Substitutions: S}

\ForAll {clause \(C \in P\)}
   \State let new\_clauses = Instantiate clause(C, S)
   \State \(P = P \cup new\_clauses\)
\EndFor

\State \(P' = \emptyset \)

\ForAll {clause \(C \in P\)}
   \If{\(C\) is ground}
      \State \(P' = P' \cup \{mangle(C)\}\)
   \EndIf
\EndFor

\State \Return \(P'\)

\EndFunction
\end{algorithmic}
\end{algorithm}

\break

% target page number: 2-4
\section{Evaluation}

Introduce context of zipperposition (do it in intro or background?).

Explain how the final bounds were chosen (i liked the way those numbers looked)

Present evaluation method (which problems were used, options used, server characteristics ect...)



Provide complete list of problems explain which ones we excluded and why: for some it was lack of axiom files, a handful of others were simply too large

Compare the following:
    \begin{itemize}
        \item e on original portfolio mode
        \item e with original portfolio mode and monomorphisation
        \item adjusted portforlio mode 

        \item when e is called
        \item using zipp only as monomorphiser (passing the result to E, vampire, satallax, leo3)

        \item test different bounds
    \end{itemize}

\break

\begin{center}
\captionof{table}{Bounds and options to test}
\begin{tabular}{ccccc}
   \toprule
   \multicolumn{5}{c}{monomorphisation point (steps)} \\
   0 & 25 & 50 & 100 & 200 \\
   \midrule
   \multicolumn{4}{c}{new clauses multiplier} \\
   1.0 & 1.5 & 2.0 & 3.0 \\
   \midrule
   \multicolumn{3}{c}{total new clauses cap} \\
   2000 & 5000 & none\\
   \midrule
   \multicolumn{4}{c}{loop number} \\
   2 & 3 & 4 & 5 \\
   \midrule
   \multicolumn{5}{c}{monomorphisation point (steps)} \\
   0 & 25 & 50 & 100 & 200 \\

   \bottomrule
\end{tabular}
\end{center}

\begin{center}
\captionof{table}{Monomorphisation to the service of Zipperposition?}
\begin{tabular}{lccc}
   \toprule
   &portfolio classic & portfolio monomorphisation & custom portfolio \\
   \midrule
   bounds  & -  & - & - \\
   bounds2 & - & - & - \\
   bounds3 & - & - & - \\
   bounds4 & - & - & - \\
   bounds5 & - & - & - \\

   \bottomrule
\end{tabular}
\end{center}

\bigskip
\bigskip
\bigskip

\begin{center}
\captionof{table}{Monomorphisation as an alternative to native polymorphism?}
\begin{tabular}{lccccccccc}
   \toprule
   & \multicolumn{2}{c}{E}   & \multicolumn{2}{c}{Vampire} &\multicolumn{2}{c}{Leo3} & \multicolumn{2}{c}{Satallax} \\
   \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
   & native & mono & native & mono & native & mono & native & mono\\
   \midrule
   bounds  & - & - & - & - & - & - & - & - \\
   bounds' & - & - & - & - & - & - & - & - \\
   bounds* & - & - & - & - & - & - & - & - \\
   bounds? & - & - & - & - & - & - & - & - \\
   \bottomrule
\end{tabular}
\end{center}

\break

% target page number: 0.5-1
\section{Related work}

\begin{itemize}
    \item part of Sascha Boehme's thesis (Proving Theorems of Higher-Order Logic with SMT Solvers)
    \item How to Encode Polymorphic Types Safely and Efﬁciently, section 4

    \item Expressing polymorphic types in a many-sorted language
    \item Encoding Monomorphic and Polymorphic Types
\end{itemize}

% target page number: 0.5-1
\section{Conclusion}

% summary 

% Future avenues

\end{document}
