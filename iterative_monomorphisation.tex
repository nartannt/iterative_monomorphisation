\documentclass{article}

\usepackage{soul}
\usepackage{caption}
\usepackage{booktabs}
\usepackage{array}
\usepackage{multirow}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsmath}
\usepackage{authblk}

\begin{document}

\title{Iterative Monomorphisation}
\author{Jasmin Blanchette, Sascha B\"ohme, Tanguy Bozec}
\affil{[add affiliation]}
\date{[date]}
\maketitle
\begin{abstract}
   [Write abstract]
\end{abstract}


% target page number: 2-2.5
\section{Introduction}
%context and motivation (automatic theorem proving, better performance)\\
Automatic theorem provers have many applications, and are particularly useful as backends to proof assistants. Speed is one of the main limitations of the use of theorem provers in this setting. Indeed, a prover can often be expected to run for no more than a few seconds for a given instance of a problem.

%what we are trying to do (monomorphisation)\\
This paper presents an iterative method, first introduced in [sacha's thesis], that allows for higher order polymorphic problems to be approximated monomorphically. This algorithm aims to acheive the following:
   \begin{itemize}
      \item allow for existing higher order monomorphic theorem provers to be extended to higher order polymorphic languages
      \item improve the performance of existing higher order polymorphic theorem provers on polymorphic problems
   \end{itemize}

It is therefore aimed at being practically useful, to this end it forgoes completeness, this is the main theoretical weakness of this algorithm.

%basic idea of the algorithm

A given problem is seen as a set of polymorphic clauses. Type variables of the problem are quantified at the clause level. The type arguments of the problem are collected and the uninstantiated type arguments are matched against those that contain no type variables. This yields new type arguments, both uninstantitated and ground, the process is then repeated making use of the newly derived type arguments.

The new type arguments that have been derived are used to create new clauses, without type variables.
    A final step is required as the newly generated problem, whilst it does not contain type variables, still makes use of type constructors. Those can be transparently erased through the mangling method presented in [encoding monomorphic and polymorphic types].

%overview of the paper

This paper will first give some general background and describe the logical framework. Then a high-level mathematical description of the algorithm will be presented. A more detailed low-level description of the algorithm corresponding to its actual implementation will follow.
       Finally, an evaluation of the algorithm's implementation in zippeposition will be discussed.

% target page number: 0.5-1.5
\section{Background}

% mention alternatives, (guards, tags) and current implementation in Isabelle

Language: TF1  %TH1?

We want our starting logic to be:
\begin{itemize}
    \item higher order
    \item polymorphic: we want atomic types, type constructors and universally quantified type variables
\end{itemize}
our target logic to be the same thing except we foregoe type constructors and type variables 
we want our target logic to be a subset of the starting logic


Definitions :
\begin{itemize}
   \item type arguments % part of the logic
   \item ground and uninstantiated types %monomorphic and polymorphic
   \item matching % being overly precise probably uncecessary
   \item term, literal, clause  % also part logic
\end{itemize}

% target page number: 2-4
% needs to be re-written
\section{High level algorithm}

We consider a problem \(P\) to be a set of clauses \(C_1, \dots, C_n\).

Algorithm :
\begin{itemize}
    \item Let \(F\) be the set of all function symbols in \(P\).
    \item Given a function symbol \(f \in F\), we define \(I(f)\), the set of all ground type arguments that are passed to \(f\) is instantiated in \(P\).

    For instance if \(f^{a \rightarrow b}\) occurs in \(P\) (with \(a\) and \(b\) ground types) then, \(a \rightarrow b \in I(f) \).
 \item Given a clause \(C \in P\) and a function symbol \(f \in C\), we define \(U(C, f)\), the set of uninstantiated type arguments passed to \(f\) in \(C\). 

    It is important to paramatrise \(U\) with the clause of \(f\) because type variables are quantified at the clause level. This is not necessary for \(I\) because its type arguments do not contain type variables. [seems a sufficient explanation, if not clear expand further]

\end{itemize}

A single iteration of the algorithm will do the following:
\begin{itemize}
   \item For each clause \(C_i \in P\) and each function symbol \(f \in C_i\), we consider the uninstantiated type arguments \(U(C_i, f)\) these type type arguments are matched against all ground type arguments of \(I(f)\)

      [We have an issue: we don't define matching, don't think we need to go into a lot of detail]

      Each such matching may return a substitution which instantiates one or more type variables of \(C_i\) (that would also be in the type arguments of \(f\))

   \item Once all such substiutions have been computed. We apply each substitutions to the clause they were generated from. These newly created clauses are added to the problem and the newly generated type arguments (both ground and uninstantiated) will be added to \(U\) and \(I\) to be used for the next iteration.
\end{itemize}

After a set number of iterations, we take all clauses with no type variables and mangle their types [see relevant paper]. The resulting set of clauses forms a problem \(P'\) with the following properties:
\begin{itemize}
   \item \(P'\) is monomorphic [rewrite that as saying \(P'\) is in the monomorphic fragment of the original logic]
   \item if a contradiction can be derived from \(P'\), then a contradiction can be derived from \(P\) [soundness]
   \item we do not however have the converse [completeness]
\end{itemize}

[Add 2 or 3 examples to show how it works and explain why we have the different steps]

\subsection{Soundness}
Soundness of the algorithm follows from the fact that the resulting clauses are obtained by directly instantiating universally quantified type variable of the initial clauses.

Then we rely upon the soundness of mangling [encoding mono and poly ty].

\subsection{Incompleteness}

This algorithm is incomplete, this is because not all possible instantiations of the type variables are generated.
Consider the following problem [note that we need a consistent notation for type arguments (notably to distinguish them from normal arguments) also this might not be an issue but they are terms not literals ]: 
\begin{align*}
   P = \{ & \forall \alpha. f \langle \alpha \rightarrow int \rangle \\
          & \forall \alpha. f \langle nat \rightarrow \alpha \rangle \\
          & \forall \alpha \beta. \lnot f \langle \alpha \rightarrow \beta \rangle \}
\end{align*}

The algorithm only matches against ground type arguments. This is the case because polymorphic type unification is undecidable (quick ref) and whilst interweaving phases of unification with the current algorithm would be theoretically possible, it would most likely not be of any practical use. Indeed, the current implementation is already very explosive and generating new substitutions with a costly unification procedure would most likely not result in any improvements.


The previous problem would therefore never instantiate a clause, despite the problem becoming trvially contradictory if the the last clause and any of the first two are instantiated.


Furthermore, the finite number of iterations of the algorithm consitutes an additional source of incompleteness in practice.
%find example that would require a list list list int to be solved but only allow such a type to be generated after 3 iterations

% target page number: 2-4
\section{Low level algorithm}

This section presents a more detailed description of the algorithm, it follows closely the implementation in Zipperposition.

The main departures from the high-level description of the algorithm are the following:
\begin{itemize}
   \item the substitutions are directly applied to the type arguments, instead of the clauses, this also dispenses us from having to re-extract the type arguments from the clauses at each iteration
   \item as a result the new clauses are generated only once all substitutions have been computed and iteratively applied to the various type arguments
   \item some operations are performed in a different order to avoid useless computations [which ones?]
   \item bounds are used at almost all steps of the process as the various enumerations are highly explosive and require bounds in order to be useful in a practical setting
   \item the various sets are implemented with maps
\end{itemize}

\begin{algorithm}[tbh]
\begin{algorithmic}[1]
\Function{Type arguments}{Literal: lit}

   \State let \(t_1\), \(t_2\) be the terms of \(lit\)
   \State recursively collect the function symbols and associated type arguments of \(t_1\) and \(t_2\) into a list of pairs

   \State \Comment Going a bit fast here, probably need to reformulate, but don't go into details, it's not important

   \State \Return the list of (function symbol, type argument) pairs

\EndFunction
\end{algorithmic}
\end{algorithm}


\begin{algorithm}[tbh]
\begin{algorithmic}[1]
\Function{Initialisation}{Problem: P}
   \ForAll{clause \(C \in P\)}
      \ForAll{literal \(lit \in C\)}
         \State let fun\_sym\_ty\_args = Type arguments(lit)
         \ForAll{(fun\_sym, ty\_args) in fun\_sym\_ty\_args}
         \If{\( \forall ty\_arg \in ty\_args. is\_ground(ty\_arg)\) }

            \State \(I(fun\_sym) = I(fun\_sym) \cup \{ty\_args\}\)

         \Else

            \State \(U(C, fun\_sym) = U(C, fun\_sym) \cup \{ty\_args\}\)

         \EndIf
         \EndFor
      \EndFor
   \EndFor

   \State \Return (I, U)

\EndFunction
\end{algorithmic}
\end{algorithm}

This function is very basic and simply gathers all the type arguments of the various function symbols of the problem and sorts them depending on whether they are ground or not.


\begin{algorithm}[tbh]
\begin{algorithmic}[1]
\Function{Iteration}{Problem: P, U, I, S}
   \ForAll {clause \(C \in P\)}
      \ForAll {\(fun\_sym, ty\_args \in U(C)\)}
      \ForAll {\( ty\_args' \in I(fun\_sym) \)}
            
            \State \(subst = match(ty\_args, ty\_arg')\)

            \State \Comment the match may fail in which case we simply continue with the loop

            \State \(S(C) = S(C) \cup \{subst\}\)
         \EndFor
      \EndFor
   
   \EndFor

   \ForAll {clause \(C \in P\)}
      \ForAll {\(subst \in S(C)\)}
         \ForAll {\(fun\_symbol, ty\_args \in U(C)\)}
            \State \(ty\_args' = apply(subst, ty\_args)\)

            \If {\( \forall ty\_arg \in ty\_args'. is\_ground(ty\_arg)\) }

               \State \(I(fun\_sym) = I(fun\_sym) \cup \{ty\_args'\}\)
            \Else

               \State \(U(C, fun\_sym) = U(C, fun\_sym) \cup \{ty\_args'\}\)
            \EndIf

         \EndFor
      \EndFor
   \EndFor

   \State \Return (I, U, S)

\EndFunction
\end{algorithmic}
\end{algorithm}
In the Zipperposition implemntation this iteration procedure is limited by several bounds. The number of new type arguments that can be generated for each function symbol is limited by user-defined bounds. These bounds can be defined proportionally. Additionally, the number of new type arguments can be limited at the clause level. In both cases, the bounds for the new ground type arguments and non-ground type arguments can be set independently.

A consequences of these bounds is that in order to avoid unecessary computations, the program will iteratively apply substitutions until either the ground or non-ground bound is reached. Then substitutions can be discarded depending on whether they instantiate a given set of type arguments or not.

Additionally, there are bounds for the number of substitutions that are generated.

\begin{algorithm}[tbh]
\begin{algorithmic}[1]
\Function{Instantiate clause}{Clause: C, Substitutions S}

   \If{\(C\) is ground}

      \State \Return \(\{C\}\)

   \Else

      \State let \(v\) be a type variable in \(C\)
      \State let new\_clauses be an empty set of clauses

      \ForAll{subst \(\in S(C)\)}

         \If{subst instantiates \(v\)}

            \State \(C' = apply(subst, C)\)

            \State let v\_clauses = Instantiate clause(C', S)

         \EndIf

         \State \( new\_clauses = new\_clauses \cup v\_clauses\)

      \EndFor

      \State \Return new\_clauses
   \EndIf

\EndFunction
\end{algorithmic}
\end{algorithm}
This function differs from the implementation in a significant way, indeed, through experimentation it was found that the generated clauses were far more likely to be solved by E if the substitutions that are combined to instantiate the clause were derived from the same iteration. In practice, we therefore store the iteration at which each substition is generated and at the instantiation phase only apply substitutions from the same iteration.


What the previous function does is essentially going through the tree of possible substitutions that instantiate the clause and collecting all possible instantiations. This is extremely explosive, and a hard cap on the number of clauses we want to generate is almost indispensable for any implementation.

\begin{algorithm}[tbh]
\begin{algorithmic}[1]
\Function{Generate clauses}{Problem: P, Substitutions: S}

\ForAll {clause \(C \in P\)}
   \State let new\_clauses = Instantiate clause(C, S)
   \State \(P = P \cup new\_clauses\)
\EndFor

\State \(P' = \emptyset \)

\ForAll {clause \(C \in P\)}
   \If{\(C\) is ground}
      \State \(P' = P' \cup \{mangle(C)\}\)
   \EndIf
\EndFor

\State \Return \(P'\)

\EndFunction
\end{algorithmic}
\end{algorithm}

Implementation in Zipperposition.

% target page number: 2-4
\section{Evaluation}

% Provide complete list of problems explain which ones were excluded and why

Compare the following:
    \begin{itemize}
        \item E on original portfolio mode
        \item E with original portfolio mode and monomorphisation
        \item adjusted portforlio mode 

        \item when E is called
        \item using zipp only as monomorphiser (passing the result to E, Vampire, Satallax, Leo-III)

        \item test different bounds
    \end{itemize}

\break


We ignore the following bounds: ty var limit per clause, subst per ty var, limiting the new type args per clause

\subsection{Parameter optimisation}

Considering the total number of parameters, it would be impractical to exhaustively test all option combinations. Consequently, options have been grouped together into sets of 2 or 3 options that will be more thouroughly tested. Two options will be part of the same group if it is deemed likely that a change in one of the options will significantly impact how much a change in the second option will affect the overall results.

The first two groups of options are natural as they affect how many new monomorphic and polymorphic type arguments are generated respectively for each function symbol at each iteration. These options are closely linked and must be set together if they are to be relevant.

During an iteration, for each function symbol (for each clause in the polymorphic case), the number of newly generated type arguments is determined by this formula:
\( \min(\text{cap}, \max(\text{floor}, \text{multiplier} \times \text{number of type arguments})) \)

The multiplier allows for a controlled increase of the number of type arguments, relative to the size of the problem whilst the floor prevents some function symbols seeing no new type arguments if the multiplier is too low.
The cap acts as a final limit in the case the explosion is too fast despite the multiplier bound.

\begin{table}[H]
\caption{Monomorphic type argument generation}
\centering\begin{tabular}{@{}llc*{9}{>{\centering\arraybackslash}p{1.5em}}@{}}
   && \multicolumn{3}{c}{floor 1} & \multicolumn{3}{c}{floor 3} & \multicolumn{3}{c}{floor 9}\\
   \cmidrule(lr){3-5} \cmidrule(lr){6-8} \cmidrule(lr){9-11}
   && \multicolumn{3}{c}{multiplier} & \multicolumn{3}{c}{multiplier} & \multicolumn{3}{c}{multiplier}\\
   && 0.0 & 1.0 & 2.0 & 0.0 & 1.0 & 2.0 & 0.0 & 1.0 & 2.0\\
   \cmidrule(lr){3-5} \cmidrule(lr){6-8} \cmidrule(lr){9-11}
   \multirow{4}{1em}{cap}
   & \multicolumn{1}{l|}{25} & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
   & \multicolumn{1}{l|}{50} & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
   &\multicolumn{1}{l|}{100} & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
   &\multicolumn{1}{l|}{none}& 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
\end{tabular}
\end{table}

\bigskip
\bigskip

If the floor and multiplier are set to 0 and 0.0, no new polymorphic type arguments will be generated, as a result the cap will be irrelevant.

\begin{table}[H]
\caption{Polymorphic type argument generation}
\centering\begin{tabular}{@{}llc*{9}{>{\centering\arraybackslash}p{1.5em}}@{}}
   && \multicolumn{3}{c}{floor 0} & \multicolumn{3}{c}{floor 3} & \multicolumn{3}{c}{floor 6}\\
   \cmidrule(lr){3-5} \cmidrule(lr){6-8} \cmidrule(lr){9-11}
   && \multicolumn{3}{c}{multiplier} & \multicolumn{3}{c}{multiplier} & \multicolumn{3}{c}{multiplier}\\
   && 0.0 & 0.5 & 1.0 & 0.0 & 0.5 & 1.0 & 0.0 & 0.5 & 1.0\\
   \cmidrule(lr){3-5} \cmidrule(lr){6-8} \cmidrule(lr){9-11}
   \multirow{4}{1em}{cap}
   & \multicolumn{1}{l|}{10} & \multirow{4}{1em}{0} & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
   & \multicolumn{1}{l|}{20}  && 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
   &\multicolumn{1}{l|}{50}  && 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
   &\multicolumn{1}{l|}{none} && 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
\end{tabular}
\end{table}

This table groups two bounds that directly affect the number of clauses generated, a first one that allows at most a number of new clauses equal to ``multiplier" times the original number of clauses. And the cap that gives an absolute bound over the total number of clauses generated.

The choice to also include e-timeout in this table is due to the fact that E being successful in solving a monomorphised problem in a certain time limit is highly dependent on the number of clauses it is given.

\begin{table}[H]
\caption{Clause generation}
\centering\begin{tabular}{@{}llc*{9}{>{\centering\arraybackslash}p{1.5em}}@{}}
   && \multicolumn{3}{c}{500 cap} & \multicolumn{3}{c}{2000 cap} & \multicolumn{3}{c}{no cap}\\
   \cmidrule(lr){3-5} \cmidrule(lr){6-8} \cmidrule(lr){9-11}
   && \multicolumn{3}{c}{multiplier} & \multicolumn{3}{c}{multiplier} & \multicolumn{3}{c}{multiplier}\\
   && 1.0 & 2.0 & 3.0 & 1.0 & 2.0 & 3.0 & 1.0 & 2.0 & 3.0\\
   \cmidrule(lr){3-5} \cmidrule(lr){6-8} \cmidrule(lr){9-11}
   \multirow{4}{4.2em}{e-timeout}
   & \multicolumn{1}{l|}{1 s} & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
   & \multicolumn{1}{l|}{5 s} & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
   &\multicolumn{1}{l|}{10 s} & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
   &\multicolumn{1}{l|}{none} & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
\end{tabular}
\end{table}

\begin{table}[H]
\caption{Substitution generation}
\centering\begin{tabular}{@{}llc*{4}{>{\centering\arraybackslash}p{1.5em}}@{}}
   \toprule
   & & \multicolumn{4}{c}{mono subst per clause}\\
   & & 2 & 5 & 7 & 10\\
   \cmidrule(lr){3-6}
   \multirow{4}{4.1em}{subst cap}
   & \multicolumn{1}{l|}{50}   & 0 & 0 & 0 & 0\\
   & \multicolumn{1}{l|}{100}  & 0 & 0 & 0 & 0\\
   & \multicolumn{1}{l|}{500}  & 0 & 0 & 0 & 0\\
   & \multicolumn{1}{l|}{none} & 0 & 0 & 0 & 0\\
   
   \bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\caption{Substitution generation}
\centering\begin{tabular}{@{}llc*{4}{>{\centering\arraybackslash}p{1.5em}}@{}}
   \toprule
   & & \multicolumn{4}{c}{e call step}\\
   & & 0 & 15 & 45 & 90\\
   \cmidrule(lr){3-6}
   \multirow{4}{3.3em}{loop nb}
   & \multicolumn{1}{l|}{2} & 0 & 0 & 0 & 0\\
   & \multicolumn{1}{l|}{3} & 0 & 0 & 0 & 0\\
   & \multicolumn{1}{l|}{4} & 0 & 0 & 0 & 0\\
   & \multicolumn{1}{l|}{5} & 0 & 0 & 0 & 0\\
   
   \bottomrule
\end{tabular}
\end{table}

\subsection{Monomorphisation as preprocessor}

\begin{table}[ht]
\caption{Evaluation of native polymorphism vs.\ monomorphisation}
\centering\begin{tabular}{@{}lccc@{}}
   \toprule
   & Native & Mono & Union \\
   \midrule
   E  &   & 0 & 0 \\
   Leo-III & 0 & 0 & 0 \\
   Satallax &  & 0 & 0 \\
   Vampire & 0 & 0 & 0 \\
   Zipperposition & 0 & 0 & 0 \\[1.5\jot]
   Total & 0 & 0 & 0 \\
   \bottomrule
\end{tabular}
\end{table}

\subsection{E as Zipperposition backend}

\begin{table}[ht]
\caption{Evaluation of Zipperposition without E vs. with E}
\centering\begin{tabular}{@{}lccc@{}}
   \toprule
   & without E & with E & Union \\
   \midrule
   Zipperposition & 0 & 0 & 0 \\
   \bottomrule
\end{tabular}
\end{table}

\break

% target page number: 0.5-1
\section{Related work}

\begin{itemize}
    \item part of Sascha Boehme's thesis (Proving Theorems of Higher-Order Logic with SMT Solvers)
    \item How to Encode Polymorphic Types Safely and Efﬁciently, section 4

    \item Expressing polymorphic types in a many-sorted language
    \item Encoding Monomorphic and Polymorphic Types
\end{itemize}

% target page number: 0.5-1
\section{Conclusion}

% summary 

% Future avenues

\end{document}
